Step:0	{"loss": {"learning_rate": 0.003}, "step": 0}
Step:0	{"loss": {"pretrain_training_loss": 4.81903136294821}, "step": 0}
Step:0	{"loss": {"pretrain_validation_loss": 4.373166629246303}, "step": 0}
Step:1	{"loss": {"learning_rate": 0.0029994859874633356}, "step": 1}
Step:1	{"loss": {"pretrain_training_loss": 4.242391710696013}, "step": 1}
Step:1	{"loss": {"pretrain_validation_loss": 4.216164588928223}, "step": 1}
Step:2	{"loss": {"learning_rate": 0.0029979443021318605}, "step": 2}
Step:2	{"loss": {"pretrain_training_loss": 3.719388661177262}, "step": 2}
Step:2	{"loss": {"pretrain_validation_loss": 4.2305149010249545}, "step": 2}
Step:3	{"loss": {"learning_rate": 0.0029953760005996923}, "step": 3}
Step:3	{"loss": {"pretrain_training_loss": 2.7674239718395732}, "step": 3}
Step:3	{"loss": {"pretrain_validation_loss": 2.4556815453938077}, "step": 3}
Step:4	{"loss": {"learning_rate": 0.00299178284305241}, "step": 4}
Step:4	{"loss": {"pretrain_training_loss": 1.7678767157637554}, "step": 4}
Step:4	{"loss": {"pretrain_validation_loss": 1.7147663746561324}, "step": 4}
Step:5	{"loss": {"learning_rate": 0.002987167292060716}, "step": 5}
Step:5	{"loss": {"pretrain_training_loss": 1.2124500702256742}, "step": 5}
Step:5	{"loss": {"pretrain_validation_loss": 1.3914715562547957}, "step": 5}
Step:6	{"loss": {"learning_rate": 0.002981532510892707}, "step": 6}
Step:6	{"loss": {"pretrain_training_loss": 0.9225667792817821}, "step": 6}
Step:6	{"loss": {"pretrain_validation_loss": 1.29055655002594}, "step": 6}
Step:7	{"loss": {"learning_rate": 0.002974882361345932}, "step": 7}
Step:7	{"loss": {"pretrain_training_loss": 0.7307665140732474}, "step": 7}
Step:7	{"loss": {"pretrain_validation_loss": 1.176017586674009}, "step": 7}
Step:8	{"loss": {"learning_rate": 0.0029672214011007086}, "step": 8}
Step:8	{"loss": {"pretrain_training_loss": 0.6110219139119853}, "step": 8}
Step:8	{"loss": {"pretrain_validation_loss": 1.1606522457940238}, "step": 8}
Step:9	{"loss": {"learning_rate": 0.002958554880596515}, "step": 9}
Step:9	{"loss": {"pretrain_training_loss": 0.5524519954038702}, "step": 9}
Step:9	{"loss": {"pretrain_validation_loss": 1.102055983883994}, "step": 9}
Step:10	{"loss": {"learning_rate": 0.0029488887394336022}, "step": 10}
Step:10	{"loss": {"pretrain_training_loss": 0.5066293542799742}, "step": 10}
Step:10	{"loss": {"pretrain_validation_loss": 1.0693948566913605}, "step": 10}
Step:11	{"loss": {"learning_rate": 0.0029382296023022893}, "step": 11}
Step:11	{"loss": {"pretrain_training_loss": 0.4351327516462492}, "step": 11}
Step:11	{"loss": {"pretrain_validation_loss": 0.9168454919542585}, "step": 11}
Step:12	{"loss": {"learning_rate": 0.0029265847744427303}, "step": 12}
Step:12	{"loss": {"pretrain_training_loss": 0.377181813120842}, "step": 12}
Step:12	{"loss": {"pretrain_validation_loss": 0.8282086508614677}, "step": 12}
Step:13	{"loss": {"learning_rate": 0.0029139622366382676}, "step": 13}
Step:13	{"loss": {"pretrain_training_loss": 0.36101754722387897}, "step": 13}
Step:13	{"loss": {"pretrain_validation_loss": 0.8660067873341697}, "step": 13}
Step:14	{"loss": {"learning_rate": 0.002900370639745802}, "step": 14}
Step:14	{"loss": {"pretrain_training_loss": 0.33008748292922974}, "step": 14}
Step:14	{"loss": {"pretrain_validation_loss": 0.8006709473473685}, "step": 14}
Step:15	{"loss": {"learning_rate": 0.0028858192987669296}, "step": 15}
Step:15	{"loss": {"pretrain_training_loss": 0.30685222570015036}, "step": 15}
Step:15	{"loss": {"pretrain_validation_loss": 0.7572673814637321}, "step": 15}
Step:16	{"loss": {"learning_rate": 0.0028703181864639005}, "step": 16}
Step:16	{"loss": {"pretrain_training_loss": 0.29298947978278866}, "step": 16}
Step:16	{"loss": {"pretrain_validation_loss": 0.7107670732906887}, "step": 16}
Step:17	{"loss": {"learning_rate": 0.0028538779265247904}, "step": 17}
Step:17	{"loss": {"pretrain_training_loss": 0.26994300471699756}, "step": 17}
Step:17	{"loss": {"pretrain_validation_loss": 0.7458384760788509}, "step": 17}
Step:18	{"loss": {"learning_rate": 0.002836509786282551}, "step": 18}
Step:18	{"loss": {"pretrain_training_loss": 0.2567382587686829}, "step": 18}
Step:18	{"loss": {"pretrain_validation_loss": 0.7073443063667842}, "step": 18}
Step:19	{"loss": {"learning_rate": 0.002818225668992947}, "step": 19}
Step:19	{"loss": {"pretrain_training_loss": 0.24484500807264578}, "step": 19}
Step:19	{"loss": {"pretrain_validation_loss": 0.6678178012371063}, "step": 19}
Step:20	{"loss": {"learning_rate": 0.0027990381056766573}, "step": 20}
Step:20	{"loss": {"pretrain_training_loss": 0.22742390114328134}, "step": 20}
Step:20	{"loss": {"pretrain_validation_loss": 0.7243443344320569}, "step": 20}
Step:21	{"loss": {"learning_rate": 0.002778960246531137}, "step": 21}
Step:21	{"loss": {"pretrain_training_loss": 0.22712669586357864}, "step": 21}
Step:21	{"loss": {"pretrain_validation_loss": 0.7028656772204808}, "step": 21}
Step:22	{"loss": {"learning_rate": 0.002758005851918135}, "step": 22}
Step:22	{"loss": {"pretrain_training_loss": 0.21673250101182773}, "step": 22}
Step:22	{"loss": {"pretrain_validation_loss": 0.6396917445319039}, "step": 22}
Step:23	{"loss": {"learning_rate": 0.0027361892829330225}, "step": 23}
Step:23	{"loss": {"pretrain_training_loss": 0.21234478801488876}, "step": 23}
Step:23	{"loss": {"pretrain_validation_loss": 0.657764311347689}, "step": 23}
Step:24	{"loss": {"learning_rate": 0.00271352549156242}, "step": 24}
Step:24	{"loss": {"pretrain_training_loss": 0.19990848458331564}, "step": 24}
Step:24	{"loss": {"pretrain_validation_loss": 0.6930086314678192}, "step": 24}
Step:25	{"loss": {"learning_rate": 0.002690030010436852}, "step": 25}
Step:25	{"loss": {"pretrain_training_loss": 0.20368035688348438}, "step": 25}
Step:25	{"loss": {"pretrain_validation_loss": 0.6124665822301593}, "step": 25}
Step:26	{"loss": {"learning_rate": 0.0026657189421854556}, "step": 26}
Step:26	{"loss": {"pretrain_training_loss": 0.19669491797685623}, "step": 26}
Step:26	{"loss": {"pretrain_validation_loss": 0.7431568631104061}, "step": 26}
Step:27	{"loss": {"learning_rate": 0.0026406089484000456}, "step": 27}
Step:27	{"loss": {"pretrain_training_loss": 0.1931116354206334}, "step": 27}
Step:27	{"loss": {"pretrain_validation_loss": 0.7732042244502476}, "step": 27}
Step:28	{"loss": {"learning_rate": 0.0026147172382160904}, "step": 28}
Step:28	{"loss": {"pretrain_training_loss": 0.19111284872759943}, "step": 28}
Step:28	{"loss": {"pretrain_validation_loss": 0.7429478083338056}, "step": 28}
Step:29	{"loss": {"learning_rate": 0.0025880615565184303}, "step": 29}
Step:29	{"loss": {"pretrain_training_loss": 0.18464181701774182}, "step": 29}
Step:29	{"loss": {"pretrain_validation_loss": 0.6587286463805607}, "step": 29}
Step:30	{"loss": {"learning_rate": 0.0025606601717798206}, "step": 30}
Step:30	{"loss": {"pretrain_training_loss": 0.1793416832452235}, "step": 30}
Step:30	{"loss": {"pretrain_validation_loss": 0.5906096334968295}, "step": 30}
Step:31	{"loss": {"learning_rate": 0.00253253186354063}, "step": 31}
Step:31	{"loss": {"pretrain_training_loss": 0.1779292854277984}, "step": 31}
Step:31	{"loss": {"pretrain_validation_loss": 0.5795026464121682}, "step": 31}
Step:32	{"loss": {"learning_rate": 0.002503695909538286}, "step": 32}
Step:32	{"loss": {"pretrain_training_loss": 0.17507531850234323}, "step": 32}
Step:32	{"loss": {"pretrain_validation_loss": 0.6006967765944344}, "step": 32}
Step:33	{"loss": {"learning_rate": 0.0024741720724952743}, "step": 33}
Step:33	{"loss": {"pretrain_training_loss": 0.17083100423864697}, "step": 33}
Step:33	{"loss": {"pretrain_validation_loss": 0.6182283163070679}, "step": 33}
Step:34	{"loss": {"learning_rate": 0.002443980586574755}, "step": 34}
Step:34	{"loss": {"pretrain_training_loss": 0.1651934667125992}, "step": 34}
Step:34	{"loss": {"pretrain_validation_loss": 0.6114260809762138}, "step": 34}
Step:35	{"loss": {"learning_rate": 0.00241314214351308}, "step": 35}
Step:35	{"loss": {"pretrain_training_loss": 0.15848033891423888}, "step": 35}
Step:35	{"loss": {"pretrain_validation_loss": 0.5720061553376061}, "step": 35}
Step:36	{"loss": {"learning_rate": 0.002381677878438709}, "step": 36}
Step:36	{"loss": {"pretrain_training_loss": 0.15619918946986613}, "step": 36}
Step:36	{"loss": {"pretrain_validation_loss": 0.5624615124293736}, "step": 36}
Step:37	{"loss": {"learning_rate": 0.0023496093553872486}, "step": 37}
Step:37	{"loss": {"pretrain_training_loss": 0.1503782424589862}, "step": 37}
Step:37	{"loss": {"pretrain_validation_loss": 0.5463500193187169}, "step": 37}
Step:38	{"loss": {"learning_rate": 0.00231695855252254}, "step": 38}
Step:38	{"loss": {"pretrain_training_loss": 0.14712183388031047}, "step": 38}
Step:38	{"loss": {"pretrain_validation_loss": 0.49892809774194447}, "step": 38}
Step:39	{"loss": {"learning_rate": 0.0022837478470739223}, "step": 39}
Step:39	{"loss": {"pretrain_training_loss": 0.14646899441014166}, "step": 39}
Step:39	{"loss": {"pretrain_validation_loss": 0.5285957434347698}, "step": 39}
Step:40	{"loss": {"learning_rate": 0.002249999999999999}, "step": 40}
Step:40	{"loss": {"pretrain_training_loss": 0.13764130745245062}, "step": 40}
Step:40	{"loss": {"pretrain_validation_loss": 0.5165419493402753}, "step": 40}
Step:41	{"loss": {"learning_rate": 0.002215738140389412}, "step": 41}
Step:41	{"loss": {"pretrain_training_loss": 0.13424429815748465}, "step": 41}
Step:41	{"loss": {"pretrain_validation_loss": 0.4949467693056379}, "step": 41}
Step:42	{"loss": {"learning_rate": 0.0021809857496093194}, "step": 42}
Step:42	{"loss": {"pretrain_training_loss": 0.1322863965254763}, "step": 42}
Step:42	{"loss": {"pretrain_validation_loss": 0.47132316870348795}, "step": 42}
Step:43	{"loss": {"learning_rate": 0.002145766645212442}, "step": 43}
Step:43	{"loss": {"pretrain_training_loss": 0.12765221913223682}, "step": 43}
Step:43	{"loss": {"pretrain_validation_loss": 0.4690036582095282}, "step": 43}
Step:44	{"loss": {"learning_rate": 0.0021101049646136995}, "step": 44}
Step:44	{"loss": {"pretrain_training_loss": 0.12197837444103282}, "step": 44}
Step:44	{"loss": {"pretrain_validation_loss": 0.42212321715695517}, "step": 44}
Step:45	{"loss": {"learning_rate": 0.0020740251485476336}, "step": 45}
Step:45	{"loss": {"pretrain_training_loss": 0.12000816979486009}, "step": 45}
Step:45	{"loss": {"pretrain_validation_loss": 0.42344326845237185}, "step": 45}
Step:46	{"loss": {"learning_rate": 0.0020375519243179497}, "step": 46}
Step:46	{"loss": {"pretrain_training_loss": 0.117417656371127}, "step": 46}
Step:46	{"loss": {"pretrain_validation_loss": 0.43504404383046286}, "step": 46}
Step:47	{"loss": {"learning_rate": 0.0020007102888506554}, "step": 47}
Step:47	{"loss": {"pretrain_training_loss": 0.11054011223756749}, "step": 47}
Step:47	{"loss": {"pretrain_validation_loss": 0.4379670811550958}, "step": 47}
Step:48	{"loss": {"learning_rate": 0.0019635254915624204}, "step": 48}
Step:48	{"loss": {"pretrain_training_loss": 0.11069460423744243}, "step": 48}
Step:48	{"loss": {"pretrain_validation_loss": 0.48428068842206684}, "step": 48}
Step:49	{"loss": {"learning_rate": 0.0019260230170558834}, "step": 49}
Step:49	{"loss": {"pretrain_training_loss": 0.10028393728577573}, "step": 49}
Step:49	{"loss": {"pretrain_validation_loss": 0.45385952506746563}, "step": 49}
Step:50	{"loss": {"learning_rate": 0.0018882285676537801}, "step": 50}
Step:50	{"loss": {"pretrain_training_loss": 0.10406644771928372}, "step": 50}
Step:50	{"loss": {"pretrain_validation_loss": 0.40099034351961954}, "step": 50}
Step:51	{"loss": {"learning_rate": 0.0018501680457838573}, "step": 51}
Step:51	{"loss": {"pretrain_training_loss": 0.09919219253503758}, "step": 51}
Step:51	{"loss": {"pretrain_validation_loss": 0.4437630921602249}, "step": 51}
Step:52	{"loss": {"learning_rate": 0.0018118675362266377}, "step": 52}
Step:52	{"loss": {"pretrain_training_loss": 0.09811622665628143}, "step": 52}
Step:52	{"loss": {"pretrain_validation_loss": 0.38698339249406544}, "step": 52}
Step:53	{"loss": {"learning_rate": 0.0017733532882382203}, "step": 53}
Step:53	{"loss": {"pretrain_training_loss": 0.09946038217648216}, "step": 53}
Step:53	{"loss": {"pretrain_validation_loss": 0.39633457149778095}, "step": 53}
Step:54	{"loss": {"learning_rate": 0.0017346516975603455}, "step": 54}
Step:54	{"loss": {"pretrain_training_loss": 0.09576764799978423}, "step": 54}
Step:54	{"loss": {"pretrain_validation_loss": 0.3987913110426494}, "step": 54}
Step:55	{"loss": {"learning_rate": 0.0016957892883300765}, "step": 55}
Step:55	{"loss": {"pretrain_training_loss": 0.0930280197897683}, "step": 55}
Step:55	{"loss": {"pretrain_validation_loss": 0.3943516122443335}, "step": 55}
Step:56	{"loss": {"learning_rate": 0.0016567926949014795}, "step": 56}
Step:56	{"loss": {"pretrain_training_loss": 0.09172544320640356}, "step": 56}
Step:56	{"loss": {"pretrain_validation_loss": 0.37844433954783846}, "step": 56}
Step:57	{"loss": {"learning_rate": 0.0016176886435917668}, "step": 57}
Step:57	{"loss": {"pretrain_training_loss": 0.08624164112236189}, "step": 57}
Step:57	{"loss": {"pretrain_validation_loss": 0.3715917744806835}, "step": 57}
Step:58	{"loss": {"learning_rate": 0.0015785039343644148}, "step": 58}
Step:58	{"loss": {"pretrain_training_loss": 0.08425088793687198}, "step": 58}
Step:58	{"loss": {"pretrain_validation_loss": 0.41982962829726084}, "step": 58}
Step:59	{"loss": {"learning_rate": 0.0015392654224618088}, "step": 59}
Step:59	{"loss": {"pretrain_training_loss": 0.08266830128496108}, "step": 59}
Step:59	{"loss": {"pretrain_validation_loss": 0.3840148150920868}, "step": 59}
Step:60	{"loss": {"learning_rate": 0.0014999999999999994}, "step": 60}
Step:60	{"loss": {"pretrain_training_loss": 0.08081720510254735}, "step": 60}
Step:60	{"loss": {"pretrain_validation_loss": 0.3597245386668614}, "step": 60}
Step:61	{"loss": {"learning_rate": 0.0014607345775381896}, "step": 61}
Step:61	{"loss": {"pretrain_training_loss": 0.08222230653400006}, "step": 61}
Step:61	{"loss": {"pretrain_validation_loss": 0.3532672588314329}, "step": 61}
Step:62	{"loss": {"learning_rate": 0.0014214960656355837}, "step": 62}
Step:62	{"loss": {"pretrain_training_loss": 0.07810674753525983}, "step": 62}
Step:62	{"loss": {"pretrain_validation_loss": 0.38345239630767275}, "step": 62}
Step:63	{"loss": {"learning_rate": 0.001382311356408232}, "step": 63}
Step:63	{"loss": {"pretrain_training_loss": 0.07888835798139157}, "step": 63}
Step:63	{"loss": {"pretrain_validation_loss": 0.3762199027197702}, "step": 63}
Step:64	{"loss": {"learning_rate": 0.0013432073050985193}, "step": 64}
Step:64	{"loss": {"pretrain_training_loss": 0.07555076447517975}, "step": 64}
Step:64	{"loss": {"pretrain_validation_loss": 0.3956024008137839}, "step": 64}
Step:65	{"loss": {"learning_rate": 0.0013042107116699223}, "step": 65}
Step:65	{"loss": {"pretrain_training_loss": 0.07492461719590685}, "step": 65}
Step:65	{"loss": {"pretrain_validation_loss": 0.37144014665058683}, "step": 65}
Step:66	{"loss": {"learning_rate": 0.001265348302439653}, "step": 66}
Step:66	{"loss": {"pretrain_training_loss": 0.07606291001581628}, "step": 66}
Step:66	{"loss": {"pretrain_validation_loss": 0.35754569300583433}, "step": 66}
Step:67	{"loss": {"learning_rate": 0.0012266467117617785}, "step": 67}
Step:67	{"loss": {"pretrain_training_loss": 0.07390693850491357}, "step": 67}
Step:67	{"loss": {"pretrain_validation_loss": 0.3656369319983891}, "step": 67}
Step:68	{"loss": {"learning_rate": 0.0011881324637733604}, "step": 68}
Step:68	{"loss": {"pretrain_training_loss": 0.07322100744299266}, "step": 68}
Step:68	{"loss": {"pretrain_validation_loss": 0.3702295592853001}, "step": 68}
Step:69	{"loss": {"learning_rate": 0.0011498319542161415}, "step": 69}
Step:69	{"loss": {"pretrain_training_loss": 0.07007421293984288}, "step": 69}
Step:69	{"loss": {"pretrain_validation_loss": 0.3529498449393681}, "step": 69}
Step:70	{"loss": {"learning_rate": 0.0011117714323462186}, "step": 70}
Step:70	{"loss": {"pretrain_training_loss": 0.07068742965550526}, "step": 70}
Step:70	{"loss": {"pretrain_validation_loss": 0.3969548600060599}, "step": 70}
Step:71	{"loss": {"learning_rate": 0.0010739769829441156}, "step": 71}
Step:71	{"loss": {"pretrain_training_loss": 0.06984846957999727}, "step": 71}
Step:71	{"loss": {"pretrain_validation_loss": 0.38066864226545605}, "step": 71}
Step:72	{"loss": {"learning_rate": 0.0010364745084375786}, "step": 72}
Step:72	{"loss": {"pretrain_training_loss": 0.06685186404248943}, "step": 72}
Step:72	{"loss": {"pretrain_validation_loss": 0.371043215904917}, "step": 72}
Step:73	{"loss": {"learning_rate": 0.0009992897111493434}, "step": 73}
Step:73	{"loss": {"pretrain_training_loss": 0.06664714217185974}, "step": 73}
Step:73	{"loss": {"pretrain_validation_loss": 0.3764801344701222}, "step": 73}
Step:74	{"loss": {"learning_rate": 0.0009624480756820492}, "step": 74}
Step:74	{"loss": {"pretrain_training_loss": 0.06663752841236799}, "step": 74}
Step:74	{"loss": {"pretrain_validation_loss": 0.35839447166238514}, "step": 74}
Step:75	{"loss": {"learning_rate": 0.000925974851452365}, "step": 75}
Step:75	{"loss": {"pretrain_training_loss": 0.06759677819259789}, "step": 75}
Step:75	{"loss": {"pretrain_validation_loss": 0.35554163583687376}, "step": 75}
Step:76	{"loss": {"learning_rate": 0.0008898950353862994}, "step": 76}
Step:76	{"loss": {"pretrain_training_loss": 0.06467430479824543}, "step": 76}
Step:76	{"loss": {"pretrain_validation_loss": 0.3484165349176952}, "step": 76}
Step:77	{"loss": {"learning_rate": 0.0008542333547875573}, "step": 77}
Step:77	{"loss": {"pretrain_training_loss": 0.06762044893010803}, "step": 77}
Step:77	{"loss": {"pretrain_validation_loss": 0.3479394039937428}, "step": 77}
Step:78	{"loss": {"learning_rate": 0.0008190142503906795}, "step": 78}
Step:78	{"loss": {"pretrain_training_loss": 0.06362817055829194}, "step": 78}
Step:78	{"loss": {"pretrain_validation_loss": 0.3143055396420615}, "step": 78}
Step:79	{"loss": {"learning_rate": 0.000784261859610587}, "step": 79}
Step:79	{"loss": {"pretrain_training_loss": 0.061655238918636154}, "step": 79}
Step:79	{"loss": {"pretrain_validation_loss": 0.3381325166140284}, "step": 79}
Step:80	{"loss": {"learning_rate": 0.00075}, "step": 80}
Step:80	{"loss": {"pretrain_training_loss": 0.06257312906825024}, "step": 80}
Step:80	{"loss": {"pretrain_validation_loss": 0.3363738102572305}, "step": 80}
Step:81	{"loss": {"learning_rate": 0.0007162521529260764}, "step": 81}
Step:81	{"loss": {"pretrain_training_loss": 0.06176635096578494}, "step": 81}
Step:81	{"loss": {"pretrain_validation_loss": 0.3502518768821444}, "step": 81}
Step:82	{"loss": {"learning_rate": 0.0006830414474774595}, "step": 82}
Step:82	{"loss": {"pretrain_training_loss": 0.06308223784941694}, "step": 82}
Step:82	{"loss": {"pretrain_validation_loss": 0.3335171469620296}, "step": 82}
Step:83	{"loss": {"learning_rate": 0.00065039064461275}, "step": 83}
Step:83	{"loss": {"pretrain_training_loss": 0.06225322486589784}, "step": 83}
Step:83	{"loss": {"pretrain_validation_loss": 0.34782916733196806}, "step": 83}
Step:84	{"loss": {"learning_rate": 0.0006183221215612901}, "step": 84}
Step:84	{"loss": {"pretrain_training_loss": 0.060849494179305824}, "step": 84}
Step:84	{"loss": {"pretrain_validation_loss": 0.31671367053474697}, "step": 84}
Step:85	{"loss": {"learning_rate": 0.0005868578564869187}, "step": 85}
Step:85	{"loss": {"pretrain_training_loss": 0.059749133156045624}, "step": 85}
Step:85	{"loss": {"pretrain_validation_loss": 0.3316460187946047}, "step": 85}
Step:86	{"loss": {"learning_rate": 0.0005560194134252438}, "step": 86}
Step:86	{"loss": {"pretrain_training_loss": 0.05899837491628916}, "step": 86}
Step:86	{"loss": {"pretrain_validation_loss": 0.33759285296712604}, "step": 86}
Step:87	{"loss": {"learning_rate": 0.0005258279275047245}, "step": 87}
Step:87	{"loss": {"pretrain_training_loss": 0.058946272439282875}, "step": 87}
Step:87	{"loss": {"pretrain_validation_loss": 0.3545997568539211}, "step": 87}
Step:88	{"loss": {"learning_rate": 0.0004963040904617129}, "step": 88}
Step:88	{"loss": {"pretrain_training_loss": 0.05875719873153645}, "step": 88}
Step:88	{"loss": {"pretrain_validation_loss": 0.3329003815140043}, "step": 88}
Step:89	{"loss": {"learning_rate": 0.00046746813645936856}, "step": 89}
Step:89	{"loss": {"pretrain_training_loss": 0.05665310966255872}, "step": 89}
Step:89	{"loss": {"pretrain_validation_loss": 0.30697791491236004}, "step": 89}
Step:90	{"loss": {"learning_rate": 0.00043933982822017855}, "step": 90}
Step:90	{"loss": {"pretrain_training_loss": 0.05708552581136641}, "step": 90}
Step:90	{"loss": {"pretrain_validation_loss": 0.3422132964645113}, "step": 90}
Step:91	{"loss": {"learning_rate": 0.0004119384434815683}, "step": 91}
Step:91	{"loss": {"pretrain_training_loss": 0.057689120831049004}, "step": 91}
Step:91	{"loss": {"pretrain_validation_loss": 0.33515461002077374}, "step": 91}
Step:92	{"loss": {"learning_rate": 0.0003852827617839086}, "step": 92}
Step:92	{"loss": {"pretrain_training_loss": 0.057208508900974106}, "step": 92}
Step:92	{"loss": {"pretrain_validation_loss": 0.31821690074035097}, "step": 92}
Step:93	{"loss": {"learning_rate": 0.00035939105159995345}, "step": 93}
Step:93	{"loss": {"pretrain_training_loss": 0.05793388762875744}, "step": 93}
Step:93	{"loss": {"pretrain_validation_loss": 0.32253033348492216}, "step": 93}
Step:94	{"loss": {"learning_rate": 0.00033428105781454354}, "step": 94}
Step:94	{"loss": {"pretrain_training_loss": 0.0572415309269791}, "step": 94}
Step:94	{"loss": {"pretrain_validation_loss": 0.32468371625457493}, "step": 94}
Step:95	{"loss": {"learning_rate": 0.000309969989563147}, "step": 95}
Step:95	{"loss": {"pretrain_training_loss": 0.05708712574256503}, "step": 95}
Step:95	{"loss": {"pretrain_validation_loss": 0.33091660482542856}, "step": 95}
Step:96	{"loss": {"learning_rate": 0.00028647450843757887}, "step": 96}
Step:96	{"loss": {"pretrain_training_loss": 0.05634483913688556}, "step": 96}
Step:96	{"loss": {"pretrain_validation_loss": 0.3289130713258471}, "step": 96}
Step:97	{"loss": {"learning_rate": 0.00026381071706697666}, "step": 97}
Step:97	{"loss": {"pretrain_training_loss": 0.0565253679195176}, "step": 97}
Step:97	{"loss": {"pretrain_validation_loss": 0.32516002122844967}, "step": 97}
Step:98	{"loss": {"learning_rate": 0.00024199414808186398}, "step": 98}
Step:98	{"loss": {"pretrain_training_loss": 0.05569170909407346}, "step": 98}
Step:98	{"loss": {"pretrain_validation_loss": 0.2976109023605074}, "step": 98}
Step:99	{"loss": {"learning_rate": 0.00022103975346886163}, "step": 99}
Step:99	{"loss": {"pretrain_training_loss": 0.056670867149596627}, "step": 99}
Step:99	{"loss": {"pretrain_validation_loss": 0.30904647388628553}, "step": 99}
Step:100	{"loss": {"learning_rate": 0.00020096189432334185}, "step": 100}
Step:100	{"loss": {"pretrain_training_loss": 0.053960853782684906}, "step": 100}
Step:100	{"loss": {"pretrain_validation_loss": 0.36507915386131834}, "step": 100}
Step:101	{"loss": {"learning_rate": 0.000181774331007052}, "step": 101}
Step:101	{"loss": {"pretrain_training_loss": 0.05414937566155973}, "step": 101}
Step:101	{"loss": {"pretrain_validation_loss": 0.34242107612746103}, "step": 101}
Step:102	{"loss": {"learning_rate": 0.00016349021371744825}, "step": 102}
Step:102	{"loss": {"pretrain_training_loss": 0.053051437372746674}, "step": 102}
Step:102	{"loss": {"pretrain_validation_loss": 0.3311541665877615}, "step": 102}
Step:103	{"loss": {"learning_rate": 0.00014612207347520934}, "step": 103}
Step:103	{"loss": {"pretrain_training_loss": 0.05360534228384495}, "step": 103}
Step:103	{"loss": {"pretrain_validation_loss": 0.32283041306904386}, "step": 103}
Step:104	{"loss": {"learning_rate": 0.0001296818135360985}, "step": 104}
Step:104	{"loss": {"pretrain_training_loss": 0.05173547120521898}, "step": 104}
Step:104	{"loss": {"pretrain_validation_loss": 0.33298157900571823}, "step": 104}
Step:105	{"loss": {"learning_rate": 0.00011418070123306987}, "step": 105}
Step:105	{"loss": {"pretrain_training_loss": 0.054736444564617195}, "step": 105}
Step:105	{"loss": {"pretrain_validation_loss": 0.31830078682729174}, "step": 105}
Step:106	{"loss": {"learning_rate": 9.962936025419735e-05}, "step": 106}
Step:106	{"loss": {"pretrain_training_loss": 0.05256789242443831}, "step": 106}
Step:106	{"loss": {"pretrain_validation_loss": 0.32272225831236157}, "step": 106}
Step:107	{"loss": {"learning_rate": 8.60377633617325e-05}, "step": 107}
Step:107	{"loss": {"pretrain_training_loss": 0.054451067162596664}, "step": 107}
Step:107	{"loss": {"pretrain_validation_loss": 0.3221154191664287}, "step": 107}
Step:108	{"loss": {"learning_rate": 7.341522555726968e-05}, "step": 108}
Step:108	{"loss": {"pretrain_training_loss": 0.05189354037461073}, "step": 108}
Step:108	{"loss": {"pretrain_validation_loss": 0.3274584008114679}, "step": 108}
Step:109	{"loss": {"learning_rate": 6.177039769771057e-05}, "step": 109}
Step:109	{"loss": {"pretrain_training_loss": 0.054605038188721824}, "step": 109}
Step:109	{"loss": {"pretrain_validation_loss": 0.3201060550553458}, "step": 109}
Step:110	{"loss": {"learning_rate": 5.111126056639751e-05}, "step": 110}
Step:110	{"loss": {"pretrain_training_loss": 0.05325728727747565}, "step": 110}
Step:110	{"loss": {"pretrain_validation_loss": 0.31730047719819204}, "step": 110}
Step:111	{"loss": {"learning_rate": 4.144511940348515e-05}, "step": 111}
Step:111	{"loss": {"pretrain_training_loss": 0.05245581803762395}, "step": 111}
Step:111	{"loss": {"pretrain_validation_loss": 0.3348734825849533}, "step": 111}
Step:112	{"loss": {"learning_rate": 3.277859889929146e-05}, "step": 112}
Step:112	{"loss": {"pretrain_training_loss": 0.05304642904387868}, "step": 112}
Step:112	{"loss": {"pretrain_validation_loss": 0.3144217569913183}, "step": 112}
Step:113	{"loss": {"learning_rate": 2.5117638654068232e-05}, "step": 113}
Step:113	{"loss": {"pretrain_training_loss": 0.053962520364186035}, "step": 113}
Step:113	{"loss": {"pretrain_validation_loss": 0.2994279403771673}, "step": 113}
Step:114	{"loss": {"learning_rate": 1.8467489107293506e-05}, "step": 114}
Step:114	{"loss": {"pretrain_training_loss": 0.053922050511059555}, "step": 114}
Step:114	{"loss": {"pretrain_validation_loss": 0.3077670484781265}, "step": 114}
Step:115	{"loss": {"learning_rate": 1.2832707939284424e-05}, "step": 115}
Step:115	{"loss": {"pretrain_training_loss": 0.05331018437509951}, "step": 115}
Step:115	{"loss": {"pretrain_validation_loss": 0.3262777860675539}, "step": 115}
Step:116	{"loss": {"learning_rate": 8.217156947589898e-06}, "step": 116}
Step:116	{"loss": {"pretrain_training_loss": 0.05389506151170834}, "step": 116}
Step:116	{"loss": {"pretrain_validation_loss": 0.3140408396720886}, "step": 116}
Step:117	{"loss": {"learning_rate": 4.623999400308054e-06}, "step": 117}
Step:117	{"loss": {"pretrain_training_loss": 0.0520609177487052}, "step": 117}
Step:117	{"loss": {"pretrain_validation_loss": 0.32242160396916525}, "step": 117}
Step:118	{"loss": {"learning_rate": 2.05569786813925e-06}, "step": 118}
Step:118	{"loss": {"pretrain_training_loss": 0.052306020956324493}, "step": 118}
Step:118	{"loss": {"pretrain_validation_loss": 0.3057647230369704}, "step": 118}
Step:119	{"loss": {"learning_rate": 5.140125366641102e-07}, "step": 119}
Step:119	{"loss": {"pretrain_training_loss": 0.05366615157412446}, "step": 119}
Step:119	{"loss": {"pretrain_validation_loss": 0.31736656384808676}, "step": 119}
Step:0	{"loss": {"learning_rate": 0.003}, "step": 0}
Step:0	{"loss": {"pretrain_training_loss": 4.9063416134227404}, "step": 0}
Step:0	{"loss": {"pretrain_validation_loss": 6.516687975989448}, "step": 0}
Step:1	{"loss": {"learning_rate": 0.0029994859874633356}, "step": 1}
Step:1	{"loss": {"pretrain_training_loss": 4.347871689362959}, "step": 1}
Step:1	{"loss": {"pretrain_validation_loss": 7.179574595557319}, "step": 1}
Step:2	{"loss": {"learning_rate": 0.0029979443021318605}, "step": 2}
Step:2	{"loss": {"pretrain_training_loss": 4.051042621785944}, "step": 2}
Step:2	{"loss": {"pretrain_validation_loss": 6.505247645907932}, "step": 2}
Step:3	{"loss": {"learning_rate": 0.0029953760005996923}, "step": 3}
Step:3	{"loss": {"pretrain_training_loss": 3.900927764719183}, "step": 3}
Step:3	{"loss": {"pretrain_validation_loss": 7.40612440639072}, "step": 3}
Step:4	{"loss": {"learning_rate": 0.00299178284305241}, "step": 4}
Step:4	{"loss": {"pretrain_training_loss": 3.6759778196161443}, "step": 4}
Step:4	{"loss": {"pretrain_validation_loss": 8.868887901306152}, "step": 4}
Step:5	{"loss": {"learning_rate": 0.002987167292060716}, "step": 5}
Step:5	{"loss": {"pretrain_training_loss": 3.453981117768721}, "step": 5}
Step:5	{"loss": {"pretrain_validation_loss": 6.905587037404378}, "step": 5}
Step:6	{"loss": {"learning_rate": 0.002981532510892707}, "step": 6}
Step:6	{"loss": {"pretrain_training_loss": 3.3614750428633258}, "step": 6}
Step:6	{"loss": {"pretrain_validation_loss": 7.72881899939643}, "step": 6}
Step:7	{"loss": {"learning_rate": 0.002974882361345932}, "step": 7}
Step:7	{"loss": {"pretrain_training_loss": 3.32793682271784}, "step": 7}
Step:7	{"loss": {"pretrain_validation_loss": 7.315172301398383}, "step": 7}
Step:8	{"loss": {"learning_rate": 0.0029672214011007086}, "step": 8}
Step:8	{"loss": {"pretrain_training_loss": 3.240204668045044}, "step": 8}
Step:8	{"loss": {"pretrain_validation_loss": 7.960434701707628}, "step": 8}
Step:9	{"loss": {"learning_rate": 0.002958554880596515}, "step": 9}
Step:9	{"loss": {"pretrain_training_loss": 3.1562370517037133}, "step": 9}
Step:9	{"loss": {"pretrain_validation_loss": 7.664585140016344}, "step": 9}
Step:10	{"loss": {"learning_rate": 0.0029488887394336022}, "step": 10}
Step:10	{"loss": {"pretrain_training_loss": 3.1108783851970325}, "step": 10}
Step:10	{"loss": {"pretrain_validation_loss": 8.224194155799019}, "step": 10}
Step:11	{"loss": {"learning_rate": 0.0029382296023022893}, "step": 11}
Step:11	{"loss": {"pretrain_training_loss": 3.0929131507873535}, "step": 11}
Step:11	{"loss": {"pretrain_validation_loss": 8.105113294389513}, "step": 11}
Step:12	{"loss": {"learning_rate": 0.0029265847744427303}, "step": 12}
Step:12	{"loss": {"pretrain_training_loss": 3.0412510308352383}, "step": 12}
Step:12	{"loss": {"pretrain_validation_loss": 8.058390617370605}, "step": 12}
Step:13	{"loss": {"learning_rate": 0.0029139622366382676}, "step": 13}
Step:13	{"loss": {"pretrain_training_loss": 3.014777491309426}, "step": 13}
Step:13	{"loss": {"pretrain_validation_loss": 7.778468343946669}, "step": 13}
Step:14	{"loss": {"learning_rate": 0.002900370639745802}, "step": 14}
Step:14	{"loss": {"pretrain_training_loss": 2.997022594105114}, "step": 14}
Step:14	{"loss": {"pretrain_validation_loss": 7.968930774264866}, "step": 14}
Step:15	{"loss": {"learning_rate": 0.0028858192987669296}, "step": 15}
Step:15	{"loss": {"pretrain_training_loss": 2.994483960758556}, "step": 15}
Step:15	{"loss": {"pretrain_validation_loss": 7.929096672270033}, "step": 15}
Step:16	{"loss": {"learning_rate": 0.0028703181864639005}, "step": 16}
Step:16	{"loss": {"pretrain_training_loss": 2.9606119459325617}, "step": 16}
Step:16	{"loss": {"pretrain_validation_loss": 7.574168973498875}, "step": 16}
Step:17	{"loss": {"learning_rate": 0.0028538779265247904}, "step": 17}
Step:17	{"loss": {"pretrain_training_loss": 2.937729493054477}, "step": 17}
Step:17	{"loss": {"pretrain_validation_loss": 8.013829973008898}, "step": 17}
Step:18	{"loss": {"learning_rate": 0.002836509786282551}, "step": 18}
Step:18	{"loss": {"pretrain_training_loss": 2.913356390866366}, "step": 18}
Step:18	{"loss": {"pretrain_validation_loss": 7.442798455556233}, "step": 18}
Step:19	{"loss": {"learning_rate": 0.002818225668992947}, "step": 19}
Step:19	{"loss": {"pretrain_training_loss": 2.891052397814664}, "step": 19}
Step:19	{"loss": {"pretrain_validation_loss": 7.8694290055169}, "step": 19}
Step:20	{"loss": {"learning_rate": 0.0027990381056766573}, "step": 20}
Step:20	{"loss": {"pretrain_training_loss": 2.8657191666689785}, "step": 20}
Step:20	{"loss": {"pretrain_validation_loss": 8.108690791659885}, "step": 20}
Step:21	{"loss": {"learning_rate": 0.002778960246531137}, "step": 21}
Step:21	{"loss": {"pretrain_training_loss": 2.860917910662564}, "step": 21}
Step:21	{"loss": {"pretrain_validation_loss": 8.147266387939453}, "step": 21}
Step:22	{"loss": {"learning_rate": 0.002758005851918135}, "step": 22}
Step:22	{"loss": {"pretrain_training_loss": 2.840304470062256}, "step": 22}
Step:22	{"loss": {"pretrain_validation_loss": 8.743348757425943}, "step": 22}
Step:0	{"loss": {"learning_rate": 0.0027361892829330225}, "step": 0}
Step:0	{"loss": {"pretrain_training_loss": 2.833938828381625}, "step": 0}
Step:0	{"loss": {"pretrain_validation_loss": 8.13496035999722}, "step": 0}
Step:1	{"loss": {"learning_rate": 0.00271352549156242}, "step": 1}
Step:1	{"loss": {"pretrain_training_loss": 2.80436203696511}, "step": 1}
Step:1	{"loss": {"pretrain_validation_loss": 8.07428534825643}, "step": 1}
Step:2	{"loss": {"learning_rate": 0.002690030010436852}, "step": 2}
Step:2	{"loss": {"pretrain_training_loss": 2.8140256491574376}, "step": 2}
Step:2	{"loss": {"pretrain_validation_loss": 8.148984034856161}, "step": 2}
Step:3	{"loss": {"learning_rate": 0.0026657189421854556}, "step": 3}
Step:3	{"loss": {"pretrain_training_loss": 2.8077429337935014}, "step": 3}
Step:3	{"loss": {"pretrain_validation_loss": 7.766172938876682}, "step": 3}
Step:4	{"loss": {"learning_rate": 0.0026406089484000456}, "step": 4}
Step:4	{"loss": {"pretrain_training_loss": 2.7635313337499445}, "step": 4}
Step:4	{"loss": {"pretrain_validation_loss": 7.7949853738149}, "step": 4}
Step:5	{"loss": {"learning_rate": 0.0026147172382160904}, "step": 5}
Step:5	{"loss": {"pretrain_training_loss": 2.764111042022705}, "step": 5}
Step:5	{"loss": {"pretrain_validation_loss": 7.839493989944458}, "step": 5}
Step:6	{"loss": {"learning_rate": 0.0025880615565184303}, "step": 6}
Step:6	{"loss": {"pretrain_training_loss": 2.7564971707083963}, "step": 6}
Step:6	{"loss": {"pretrain_validation_loss": 7.66622789700826}, "step": 6}
Step:7	{"loss": {"learning_rate": 0.0025606601717798206}, "step": 7}
Step:7	{"loss": {"pretrain_training_loss": 2.761482585560192}, "step": 7}
Step:7	{"loss": {"pretrain_validation_loss": 7.06893875863817}, "step": 7}
Step:8	{"loss": {"learning_rate": 0.00253253186354063}, "step": 8}
Step:8	{"loss": {"pretrain_training_loss": 2.758010825243863}, "step": 8}
Step:8	{"loss": {"pretrain_validation_loss": 7.2714987066056995}, "step": 8}
Step:9	{"loss": {"learning_rate": 0.002503695909538286}, "step": 9}
Step:9	{"loss": {"pretrain_training_loss": 2.752061635797674}, "step": 9}
Step:9	{"loss": {"pretrain_validation_loss": 7.942657417721218}, "step": 9}
Step:10	{"loss": {"learning_rate": 0.0024741720724952743}, "step": 10}
Step:10	{"loss": {"pretrain_training_loss": 2.7492730790918523}, "step": 10}
Step:10	{"loss": {"pretrain_validation_loss": 7.1268454392751055}, "step": 10}
Step:11	{"loss": {"learning_rate": 0.002443980586574755}, "step": 11}
Step:11	{"loss": {"pretrain_training_loss": 2.7356721531261097}, "step": 11}
Step:11	{"loss": {"pretrain_validation_loss": 8.073288122812906}, "step": 11}
Step:12	{"loss": {"learning_rate": 0.00241314214351308}, "step": 12}
Step:12	{"loss": {"pretrain_training_loss": 2.7158627900210295}, "step": 12}
Step:12	{"loss": {"pretrain_validation_loss": 7.900938987731934}, "step": 12}
Step:13	{"loss": {"learning_rate": 0.002381677878438709}, "step": 13}
Step:13	{"loss": {"pretrain_training_loss": 2.7151655067097056}, "step": 13}
Step:13	{"loss": {"pretrain_validation_loss": 7.972646660274929}, "step": 13}
Step:14	{"loss": {"learning_rate": 0.0023496093553872486}, "step": 14}
Step:14	{"loss": {"pretrain_training_loss": 2.7373901930722324}, "step": 14}
Step:14	{"loss": {"pretrain_validation_loss": 7.330346610811022}, "step": 14}
Step:15	{"loss": {"learning_rate": 0.00231695855252254}, "step": 15}
Step:15	{"loss": {"pretrain_training_loss": 2.7039360133084385}, "step": 15}
Step:15	{"loss": {"pretrain_validation_loss": 7.516892247729832}, "step": 15}
Step:16	{"loss": {"learning_rate": 0.0022837478470739223}, "step": 16}
Step:16	{"loss": {"pretrain_training_loss": 2.7010160402818157}, "step": 16}
Step:16	{"loss": {"pretrain_validation_loss": 8.17105057504442}, "step": 16}
Step:17	{"loss": {"learning_rate": 0.002249999999999999}, "step": 17}
Step:17	{"loss": {"pretrain_training_loss": 2.7139542016116054}, "step": 17}
Step:17	{"loss": {"pretrain_validation_loss": 7.502541965908474}, "step": 17}
Step:18	{"loss": {"learning_rate": 0.002215738140389412}, "step": 18}
Step:18	{"loss": {"pretrain_training_loss": 2.6905699859965932}, "step": 18}
Step:18	{"loss": {"pretrain_validation_loss": 7.125011126200358}, "step": 18}
Step:19	{"loss": {"learning_rate": 0.0021809857496093194}, "step": 19}
Step:19	{"loss": {"pretrain_training_loss": 2.68604643128135}, "step": 19}
Step:19	{"loss": {"pretrain_validation_loss": 7.247007581922743}, "step": 19}
Step:20	{"loss": {"learning_rate": 0.002145766645212442}, "step": 20}
Step:20	{"loss": {"pretrain_training_loss": 2.679339855367487}, "step": 20}
Step:20	{"loss": {"pretrain_validation_loss": 7.3061595227983265}, "step": 20}
Step:21	{"loss": {"learning_rate": 0.0021101049646136995}, "step": 21}
Step:21	{"loss": {"pretrain_training_loss": 2.6662845264781607}, "step": 21}
Step:21	{"loss": {"pretrain_validation_loss": 7.306484434339735}, "step": 21}
Step:22	{"loss": {"learning_rate": 0.0020740251485476336}, "step": 22}
Step:22	{"loss": {"pretrain_training_loss": 2.6770806616002862}, "step": 22}
Step:22	{"loss": {"pretrain_validation_loss": 7.434379551145765}, "step": 22}
Step:23	{"loss": {"learning_rate": 0.0020375519243179497}, "step": 23}
Step:23	{"loss": {"pretrain_training_loss": 2.6713469418612394}, "step": 23}
Step:23	{"loss": {"pretrain_validation_loss": 6.953523953755696}, "step": 23}
Step:24	{"loss": {"learning_rate": 0.0020007102888506554}, "step": 24}
Step:24	{"loss": {"pretrain_training_loss": 2.6814292647621847}, "step": 24}
Step:24	{"loss": {"pretrain_validation_loss": 6.583808263142903}, "step": 24}
Step:25	{"loss": {"learning_rate": 0.0019635254915624204}, "step": 25}
Step:25	{"loss": {"pretrain_training_loss": 2.6849911819804797}, "step": 25}
Step:25	{"loss": {"pretrain_validation_loss": 6.794046640396118}, "step": 25}
Step:26	{"loss": {"learning_rate": 0.0019260230170558834}, "step": 26}
Step:26	{"loss": {"pretrain_training_loss": 2.67374624339017}, "step": 26}
Step:26	{"loss": {"pretrain_validation_loss": 7.330791897243923}, "step": 26}
Step:27	{"loss": {"learning_rate": 0.0018882285676537801}, "step": 27}
Step:27	{"loss": {"pretrain_training_loss": 2.6758147846568714}, "step": 27}
Step:27	{"loss": {"pretrain_validation_loss": 7.333752287758721}, "step": 27}
Step:28	{"loss": {"learning_rate": 0.0018501680457838573}, "step": 28}
Step:28	{"loss": {"pretrain_training_loss": 2.6637429584156385}, "step": 28}
Step:28	{"loss": {"pretrain_validation_loss": 7.205221811930339}, "step": 28}
Step:29	{"loss": {"learning_rate": 0.0018118675362266377}, "step": 29}
Step:29	{"loss": {"pretrain_training_loss": 2.655309737812389}, "step": 29}
Step:29	{"loss": {"pretrain_validation_loss": 7.561603201760186}, "step": 29}
Step:30	{"loss": {"learning_rate": 0.0017733532882382203}, "step": 30}
Step:30	{"loss": {"pretrain_training_loss": 2.6428426785902546}, "step": 30}
Step:30	{"loss": {"pretrain_validation_loss": 7.283733818266127}, "step": 30}
Step:31	{"loss": {"learning_rate": 0.0017346516975603455}, "step": 31}
Step:31	{"loss": {"pretrain_training_loss": 2.660580231926658}, "step": 31}
Step:31	{"loss": {"pretrain_validation_loss": 7.4566563500298395}, "step": 31}
Step:32	{"loss": {"learning_rate": 0.0016957892883300765}, "step": 32}
Step:32	{"loss": {"pretrain_training_loss": 2.65768087126992}, "step": 32}
Step:32	{"loss": {"pretrain_validation_loss": 7.60471232732137}, "step": 32}
Step:33	{"loss": {"learning_rate": 0.0016567926949014795}, "step": 33}
Step:33	{"loss": {"pretrain_training_loss": 2.6496233983473343}, "step": 33}
Step:33	{"loss": {"pretrain_validation_loss": 7.62482762336731}, "step": 33}
Step:34	{"loss": {"learning_rate": 0.0016176886435917668}, "step": 34}
Step:34	{"loss": {"pretrain_training_loss": 2.6603491523049096}, "step": 34}
Step:34	{"loss": {"pretrain_validation_loss": 7.323079268137614}, "step": 34}
Step:35	{"loss": {"learning_rate": 0.0015785039343644148}, "step": 35}
Step:35	{"loss": {"pretrain_training_loss": 2.6460495948791505}, "step": 35}
Step:35	{"loss": {"pretrain_validation_loss": 7.206079588996039}, "step": 35}
Step:36	{"loss": {"learning_rate": 0.0015392654224618088}, "step": 36}
Step:36	{"loss": {"pretrain_training_loss": 2.6388970721851694}, "step": 36}
Step:36	{"loss": {"pretrain_validation_loss": 6.741801129447089}, "step": 36}
Step:37	{"loss": {"learning_rate": 0.0014999999999999994}, "step": 37}
Step:37	{"loss": {"pretrain_training_loss": 2.6259236509149724}, "step": 37}
Step:37	{"loss": {"pretrain_validation_loss": 7.1835953924391}, "step": 37}
Step:38	{"loss": {"learning_rate": 0.0014607345775381896}, "step": 38}
Step:38	{"loss": {"pretrain_training_loss": 2.6441899819807575}, "step": 38}
Step:38	{"loss": {"pretrain_validation_loss": 6.627494229210748}, "step": 38}
Step:39	{"loss": {"learning_rate": 0.0014214960656355837}, "step": 39}
Step:39	{"loss": {"pretrain_training_loss": 2.6152447310360993}, "step": 39}
Step:39	{"loss": {"pretrain_validation_loss": 7.2703543239169655}, "step": 39}
Step:40	{"loss": {"learning_rate": 0.001382311356408232}, "step": 40}
Step:40	{"loss": {"pretrain_training_loss": 2.619550002704967}, "step": 40}
Step:40	{"loss": {"pretrain_validation_loss": 7.107416868209839}, "step": 40}
Step:41	{"loss": {"learning_rate": 0.0013432073050985193}, "step": 41}
Step:41	{"loss": {"pretrain_training_loss": 2.641840356046503}, "step": 41}
Step:41	{"loss": {"pretrain_validation_loss": 6.8932022253672285}, "step": 41}
Step:42	{"loss": {"learning_rate": 0.0013042107116699223}, "step": 42}
Step:42	{"loss": {"pretrain_training_loss": 2.6273758498105138}, "step": 42}
Step:42	{"loss": {"pretrain_validation_loss": 7.2582699457804365}, "step": 42}
Step:43	{"loss": {"learning_rate": 0.001265348302439653}, "step": 43}
Step:43	{"loss": {"pretrain_training_loss": 2.627253211628307}, "step": 43}
Step:43	{"loss": {"pretrain_validation_loss": 7.082148128085667}, "step": 43}
Step:44	{"loss": {"learning_rate": 0.0012266467117617785}, "step": 44}
Step:44	{"loss": {"pretrain_training_loss": 2.623844062198292}, "step": 44}
Step:44	{"loss": {"pretrain_validation_loss": 7.237107700771755}, "step": 44}
Step:45	{"loss": {"learning_rate": 0.0011881324637733604}, "step": 45}
Step:45	{"loss": {"pretrain_training_loss": 2.6145342501727016}, "step": 45}
Step:45	{"loss": {"pretrain_validation_loss": 7.293164941999647}, "step": 45}
Step:46	{"loss": {"learning_rate": 0.0011498319542161415}, "step": 46}
Step:46	{"loss": {"pretrain_training_loss": 2.633892297744751}, "step": 46}
Step:46	{"loss": {"pretrain_validation_loss": 6.9248984919654}, "step": 46}
Step:47	{"loss": {"learning_rate": 0.0011117714323462186}, "step": 47}
Step:47	{"loss": {"pretrain_training_loss": 2.6196760654449465}, "step": 47}
Step:47	{"loss": {"pretrain_validation_loss": 6.993870258331299}, "step": 47}
Step:48	{"loss": {"learning_rate": 0.0010739769829441156}, "step": 48}
Step:48	{"loss": {"pretrain_training_loss": 2.6307068087837915}, "step": 48}
Step:48	{"loss": {"pretrain_validation_loss": 6.988083627488878}, "step": 48}
Step:49	{"loss": {"learning_rate": 0.0010364745084375786}, "step": 49}
Step:49	{"loss": {"pretrain_training_loss": 2.614631310376254}, "step": 49}
Step:49	{"loss": {"pretrain_validation_loss": 7.303273174497816}, "step": 49}
Step:50	{"loss": {"learning_rate": 0.0009992897111493434}, "step": 50}
Step:50	{"loss": {"pretrain_training_loss": 2.590873115712946}, "step": 50}
Step:50	{"loss": {"pretrain_validation_loss": 7.339229742685954}, "step": 50}
Step:51	{"loss": {"learning_rate": 0.0009624480756820492}, "step": 51}
Step:51	{"loss": {"pretrain_training_loss": 2.6239750190214677}, "step": 51}
Step:51	{"loss": {"pretrain_validation_loss": 7.304761542214288}, "step": 51}
Step:52	{"loss": {"learning_rate": 0.000925974851452365}, "step": 52}
Step:52	{"loss": {"pretrain_training_loss": 2.6067747896367854}, "step": 52}
Step:52	{"loss": {"pretrain_validation_loss": 6.998795217937893}, "step": 52}
Step:53	{"loss": {"learning_rate": 0.0008898950353862994}, "step": 53}
Step:53	{"loss": {"pretrain_training_loss": 2.5988408695567737}, "step": 53}
Step:53	{"loss": {"pretrain_validation_loss": 7.49881296687656}, "step": 53}
Step:54	{"loss": {"learning_rate": 0.0008542333547875573}, "step": 54}
Step:54	{"loss": {"pretrain_training_loss": 2.6116332444277677}, "step": 54}
Step:54	{"loss": {"pretrain_validation_loss": 6.993277576234606}, "step": 54}
Step:55	{"loss": {"learning_rate": 0.0008190142503906795}, "step": 55}
Step:55	{"loss": {"pretrain_training_loss": 2.600722174210982}, "step": 55}
Step:55	{"loss": {"pretrain_validation_loss": 6.829446580674913}, "step": 55}
Step:56	{"loss": {"learning_rate": 0.000784261859610587}, "step": 56}
Step:56	{"loss": {"pretrain_training_loss": 2.6086092883890326}, "step": 56}
Step:56	{"loss": {"pretrain_validation_loss": 7.292193624708387}, "step": 56}
Step:57	{"loss": {"learning_rate": 0.00075}, "step": 57}
Step:57	{"loss": {"pretrain_training_loss": 2.609413680163297}, "step": 57}
Step:57	{"loss": {"pretrain_validation_loss": 6.907002475526598}, "step": 57}
Step:58	{"loss": {"learning_rate": 0.0007162521529260764}, "step": 58}
Step:58	{"loss": {"pretrain_training_loss": 2.59549146565524}, "step": 58}
Step:58	{"loss": {"pretrain_validation_loss": 6.594221909840901}, "step": 58}
Step:59	{"loss": {"learning_rate": 0.0006830414474774595}, "step": 59}
Step:59	{"loss": {"pretrain_training_loss": 2.597675410183993}, "step": 59}
Step:59	{"loss": {"pretrain_validation_loss": 7.115801201926337}, "step": 59}
Step:60	{"loss": {"learning_rate": 0.00065039064461275}, "step": 60}
Step:60	{"loss": {"pretrain_training_loss": 2.5975443449887363}, "step": 60}
Step:60	{"loss": {"pretrain_validation_loss": 6.900054295857747}, "step": 60}
Step:61	{"loss": {"learning_rate": 0.0006183221215612901}, "step": 61}
Step:61	{"loss": {"pretrain_training_loss": 2.5845785704526034}, "step": 61}
Step:61	{"loss": {"pretrain_validation_loss": 7.092242585288154}, "step": 61}
Step:62	{"loss": {"learning_rate": 0.0005868578564869187}, "step": 62}
Step:62	{"loss": {"pretrain_training_loss": 2.590052023800937}, "step": 62}
Step:62	{"loss": {"pretrain_validation_loss": 6.858188894059923}, "step": 62}
Step:63	{"loss": {"learning_rate": 0.0005560194134252438}, "step": 63}
Step:63	{"loss": {"pretrain_training_loss": 2.589413885636763}, "step": 63}
Step:63	{"loss": {"pretrain_validation_loss": 6.929132064183553}, "step": 63}
Step:64	{"loss": {"learning_rate": 0.0005258279275047245}, "step": 64}
Step:64	{"loss": {"pretrain_training_loss": 2.5906211853027346}, "step": 64}
Step:64	{"loss": {"pretrain_validation_loss": 6.675935427347819}, "step": 64}
Step:65	{"loss": {"learning_rate": 0.0004963040904617129}, "step": 65}
Step:65	{"loss": {"pretrain_training_loss": 2.602566129511053}, "step": 65}
Step:65	{"loss": {"pretrain_validation_loss": 6.219861719343397}, "step": 65}
Step:66	{"loss": {"learning_rate": 0.00046746813645936856}, "step": 66}
Step:66	{"loss": {"pretrain_training_loss": 2.5941266905177724}, "step": 66}
Step:66	{"loss": {"pretrain_validation_loss": 6.976878457599216}, "step": 66}
Step:67	{"loss": {"learning_rate": 0.00043933982822017855}, "step": 67}
Step:67	{"loss": {"pretrain_training_loss": 2.5902777498418637}, "step": 67}
Step:67	{"loss": {"pretrain_validation_loss": 7.020240730709499}, "step": 67}
Step:68	{"loss": {"learning_rate": 0.0004119384434815683}, "step": 68}
Step:68	{"loss": {"pretrain_training_loss": 2.582739140770652}, "step": 68}
Step:68	{"loss": {"pretrain_validation_loss": 7.0304514567057295}, "step": 68}
Step:69	{"loss": {"learning_rate": 0.0003852827617839086}, "step": 69}
Step:69	{"loss": {"pretrain_training_loss": 2.593489083376798}, "step": 69}
Step:69	{"loss": {"pretrain_validation_loss": 6.757438739140828}, "step": 69}
Step:70	{"loss": {"learning_rate": 0.00035939105159995345}, "step": 70}
Step:70	{"loss": {"pretrain_training_loss": 2.5878139062361285}, "step": 70}
Step:70	{"loss": {"pretrain_validation_loss": 6.776408142513699}, "step": 70}
Step:71	{"loss": {"learning_rate": 0.00033428105781454354}, "step": 71}
Step:71	{"loss": {"pretrain_training_loss": 2.5890923630107534}, "step": 71}
Step:71	{"loss": {"pretrain_validation_loss": 6.32247871822781}, "step": 71}
Step:72	{"loss": {"learning_rate": 0.000309969989563147}, "step": 72}
Step:72	{"loss": {"pretrain_training_loss": 2.5805317271839487}, "step": 72}
Step:72	{"loss": {"pretrain_validation_loss": 6.599082363976373}, "step": 72}
Step:73	{"loss": {"learning_rate": 0.00028647450843757887}, "step": 73}
Step:73	{"loss": {"pretrain_training_loss": 2.5698542768304997}, "step": 73}
Step:73	{"loss": {"pretrain_validation_loss": 6.6443831125895185}, "step": 73}
Step:74	{"loss": {"learning_rate": 0.00026381071706697666}, "step": 74}
Step:74	{"loss": {"pretrain_training_loss": 2.587194397232749}, "step": 74}
Step:74	{"loss": {"pretrain_validation_loss": 6.783616171942817}, "step": 74}
Step:75	{"loss": {"learning_rate": 0.00024199414808186398}, "step": 75}
Step:75	{"loss": {"pretrain_training_loss": 2.5793778029355137}, "step": 75}
Step:75	{"loss": {"pretrain_validation_loss": 6.463182104958428}, "step": 75}
Step:76	{"loss": {"learning_rate": 0.00022103975346886163}, "step": 76}
Step:76	{"loss": {"pretrain_training_loss": 2.5739473451267587}, "step": 76}
Step:76	{"loss": {"pretrain_validation_loss": 6.697128322389391}, "step": 76}
Step:77	{"loss": {"learning_rate": 0.00020096189432334185}, "step": 77}
Step:77	{"loss": {"pretrain_training_loss": 2.5815406149083917}, "step": 77}
Step:77	{"loss": {"pretrain_validation_loss": 6.755595604578654}, "step": 77}
Step:78	{"loss": {"learning_rate": 0.000181774331007052}, "step": 78}
Step:78	{"loss": {"pretrain_training_loss": 2.5813678936524824}, "step": 78}
Step:78	{"loss": {"pretrain_validation_loss": 7.109526369306776}, "step": 78}
Step:79	{"loss": {"learning_rate": 0.00016349021371744825}, "step": 79}
Step:79	{"loss": {"pretrain_training_loss": 2.578342511437156}, "step": 79}
Step:79	{"loss": {"pretrain_validation_loss": 6.927525705761379}, "step": 79}
Step:80	{"loss": {"learning_rate": 0.00014612207347520934}, "step": 80}
Step:80	{"loss": {"pretrain_training_loss": 2.578530554337935}, "step": 80}
Step:80	{"loss": {"pretrain_validation_loss": 6.855079147550795}, "step": 80}
Step:81	{"loss": {"learning_rate": 0.0001296818135360985}, "step": 81}
Step:81	{"loss": {"pretrain_training_loss": 2.569694913517345}, "step": 81}
Step:81	{"loss": {"pretrain_validation_loss": 6.772403664059109}, "step": 81}
Step:82	{"loss": {"learning_rate": 0.00011418070123306987}, "step": 82}
Step:82	{"loss": {"pretrain_training_loss": 2.5684045791625976}, "step": 82}
Step:82	{"loss": {"pretrain_validation_loss": 6.871480809317695}, "step": 82}
Step:83	{"loss": {"learning_rate": 9.962936025419735e-05}, "step": 83}
Step:83	{"loss": {"pretrain_training_loss": 2.5834493030201306}, "step": 83}
Step:83	{"loss": {"pretrain_validation_loss": 6.716331561406453}, "step": 83}
Step:84	{"loss": {"learning_rate": 8.60377633617325e-05}, "step": 84}
Step:84	{"loss": {"pretrain_training_loss": 2.575809370387684}, "step": 84}
Step:84	{"loss": {"pretrain_validation_loss": 6.726715909110175}, "step": 84}
Step:85	{"loss": {"learning_rate": 7.341522555726968e-05}, "step": 85}
Step:85	{"loss": {"pretrain_training_loss": 2.577238533713601}, "step": 85}
Step:85	{"loss": {"pretrain_validation_loss": 6.6569650703006324}, "step": 85}
Step:86	{"loss": {"learning_rate": 6.177039769771057e-05}, "step": 86}
Step:86	{"loss": {"pretrain_training_loss": 2.570864204926924}, "step": 86}
Step:86	{"loss": {"pretrain_validation_loss": 6.665241771274143}, "step": 86}
Step:87	{"loss": {"learning_rate": 5.111126056639751e-05}, "step": 87}
Step:87	{"loss": {"pretrain_training_loss": 2.5724128159609707}, "step": 87}
Step:87	{"loss": {"pretrain_validation_loss": 6.93817491001553}, "step": 87}
Step:88	{"loss": {"learning_rate": 4.144511940348515e-05}, "step": 88}
Step:88	{"loss": {"pretrain_training_loss": 2.5819680604067714}, "step": 88}
Step:88	{"loss": {"pretrain_validation_loss": 6.613768842485216}, "step": 88}
Step:89	{"loss": {"learning_rate": 3.277859889929146e-05}, "step": 89}
Step:89	{"loss": {"pretrain_training_loss": 2.5777465560219506}, "step": 89}
Step:89	{"loss": {"pretrain_validation_loss": 6.777180353800456}, "step": 89}
Step:90	{"loss": {"learning_rate": 2.5117638654068232e-05}, "step": 90}
Step:90	{"loss": {"pretrain_training_loss": 2.5763038548556243}, "step": 90}
Step:90	{"loss": {"pretrain_validation_loss": 6.824773947397868}, "step": 90}
Step:91	{"loss": {"learning_rate": 1.8467489107293506e-05}, "step": 91}
Step:91	{"loss": {"pretrain_training_loss": 2.5853581016713925}, "step": 91}
Step:91	{"loss": {"pretrain_validation_loss": 6.769559966193305}, "step": 91}
Step:92	{"loss": {"learning_rate": 1.2832707939284424e-05}, "step": 92}
Step:92	{"loss": {"pretrain_training_loss": 2.562483054941351}, "step": 92}
Step:92	{"loss": {"pretrain_validation_loss": 6.664865414301555}, "step": 92}
Step:93	{"loss": {"learning_rate": 8.217156947589898e-06}, "step": 93}
Step:93	{"loss": {"pretrain_training_loss": 2.5723051331259987}, "step": 93}
Step:93	{"loss": {"pretrain_validation_loss": 6.70216088824802}, "step": 93}
Step:94	{"loss": {"learning_rate": 4.623999400308054e-06}, "step": 94}
Step:94	{"loss": {"pretrain_training_loss": 2.568451903083108}, "step": 94}
Step:94	{"loss": {"pretrain_validation_loss": 6.663524760140313}, "step": 94}
Step:95	{"loss": {"learning_rate": 2.05569786813925e-06}, "step": 95}
Step:95	{"loss": {"pretrain_training_loss": 2.5650831612673675}, "step": 95}
Step:95	{"loss": {"pretrain_validation_loss": 6.724831607606676}, "step": 95}
Step:96	{"loss": {"learning_rate": 5.140125366641102e-07}, "step": 96}
Step:96	{"loss": {"pretrain_training_loss": 2.5578126777302135}, "step": 96}
Step:96	{"loss": {"pretrain_validation_loss": 6.6706986162397595}, "step": 96}
Step:97	{"loss": {"learning_rate": 0.0}, "step": 97}
Step:97	{"loss": {"pretrain_training_loss": 2.5601752281188963}, "step": 97}
Step:97	{"loss": {"pretrain_validation_loss": 6.608462413152059}, "step": 97}
Step:98	{"loss": {"learning_rate": 5.140125366641102e-07}, "step": 98}
Step:98	{"loss": {"pretrain_training_loss": 2.556275510787964}, "step": 98}
Step:98	{"loss": {"pretrain_validation_loss": 6.793125046624078}, "step": 98}
Step:99	{"loss": {"learning_rate": 2.05569786813925e-06}, "step": 99}
Step:99	{"loss": {"pretrain_training_loss": 2.5732103326103903}, "step": 99}
Step:99	{"loss": {"pretrain_validation_loss": 6.555769734912449}, "step": 99}
Step:100	{"loss": {"learning_rate": 4.623999400308054e-06}, "step": 100}
Step:100	{"loss": {"pretrain_training_loss": 2.5770465504039417}, "step": 100}
Step:100	{"loss": {"pretrain_validation_loss": 6.819334480497572}, "step": 100}
Step:101	{"loss": {"learning_rate": 8.217156947589898e-06}, "step": 101}
Step:101	{"loss": {"pretrain_training_loss": 2.569993166490035}, "step": 101}
Step:101	{"loss": {"pretrain_validation_loss": 6.47305875354343}, "step": 101}
Step:102	{"loss": {"learning_rate": 1.2832707939284258e-05}, "step": 102}
Step:102	{"loss": {"pretrain_training_loss": 2.582724592902444}, "step": 102}
Step:102	{"loss": {"pretrain_validation_loss": 6.619220495223999}, "step": 102}
Step:103	{"loss": {"learning_rate": 1.8467489107293344e-05}, "step": 103}
Step:103	{"loss": {"pretrain_training_loss": 2.5790129141374067}, "step": 103}
Step:103	{"loss": {"pretrain_validation_loss": 6.6924064954121905}, "step": 103}
Step:104	{"loss": {"learning_rate": 2.5117638654068077e-05}, "step": 104}
Step:104	{"loss": {"pretrain_training_loss": 2.5751557133414527}, "step": 104}
Step:104	{"loss": {"pretrain_validation_loss": 6.635806587007311}, "step": 104}
Step:105	{"loss": {"learning_rate": 3.2778598899291464e-05}, "step": 105}
Step:105	{"loss": {"pretrain_training_loss": 2.572888213937933}, "step": 105}
Step:105	{"loss": {"pretrain_validation_loss": 6.692556195788914}, "step": 105}
Step:106	{"loss": {"learning_rate": 4.144511940348499e-05}, "step": 106}
Step:106	{"loss": {"pretrain_training_loss": 2.568436050415039}, "step": 106}
Step:106	{"loss": {"pretrain_validation_loss": 6.83981106016371}, "step": 106}
Step:107	{"loss": {"learning_rate": 5.111126056639735e-05}, "step": 107}
Step:107	{"loss": {"pretrain_training_loss": 2.5707524538040163}, "step": 107}
Step:107	{"loss": {"pretrain_validation_loss": 6.454176743825276}, "step": 107}
Step:108	{"loss": {"learning_rate": 6.17703976977104e-05}, "step": 108}
Step:108	{"loss": {"pretrain_training_loss": 2.573037524656816}, "step": 108}
Step:108	{"loss": {"pretrain_validation_loss": 6.71071375740899}, "step": 108}
Step:109	{"loss": {"learning_rate": 7.341522555726952e-05}, "step": 109}
Step:109	{"loss": {"pretrain_training_loss": 2.5714331648566504}, "step": 109}
Step:109	{"loss": {"pretrain_validation_loss": 6.9639111624823675}, "step": 109}
Step:110	{"loss": {"learning_rate": 8.603776336173234e-05}, "step": 110}
Step:110	{"loss": {"pretrain_training_loss": 2.5722690972414886}, "step": 110}
Step:110	{"loss": {"pretrain_validation_loss": 6.737864414850871}, "step": 110}
Step:111	{"loss": {"learning_rate": 9.96293602541972e-05}, "step": 111}
Step:111	{"loss": {"pretrain_training_loss": 2.5801406036723744}, "step": 111}
Step:111	{"loss": {"pretrain_validation_loss": 6.775605863995022}, "step": 111}
Step:112	{"loss": {"learning_rate": 0.0001141807012330697}, "step": 112}
Step:112	{"loss": {"pretrain_training_loss": 2.576450577649203}, "step": 112}
Step:112	{"loss": {"pretrain_validation_loss": 6.533893214331733}, "step": 112}
Step:113	{"loss": {"learning_rate": 0.00012968181353609866}, "step": 113}
Step:113	{"loss": {"pretrain_training_loss": 2.5784607193686746}, "step": 113}
Step:113	{"loss": {"pretrain_validation_loss": 6.60453446706136}, "step": 113}
Step:114	{"loss": {"learning_rate": 0.00014612207347520917}, "step": 114}
Step:114	{"loss": {"pretrain_training_loss": 2.572100708701394}, "step": 114}
Step:114	{"loss": {"pretrain_validation_loss": 6.5641683207617865}, "step": 114}
Step:115	{"loss": {"learning_rate": 0.00016349021371744808}, "step": 115}
Step:115	{"loss": {"pretrain_training_loss": 2.576000439036976}, "step": 115}
Step:115	{"loss": {"pretrain_validation_loss": 6.564942651324802}, "step": 115}
Step:116	{"loss": {"learning_rate": 0.00018177433100705184}, "step": 116}
Step:116	{"loss": {"pretrain_training_loss": 2.570022788914767}, "step": 116}
Step:116	{"loss": {"pretrain_validation_loss": 6.805847671296862}, "step": 116}
Step:117	{"loss": {"learning_rate": 0.00020096189432334166}, "step": 117}
Step:117	{"loss": {"pretrain_training_loss": 2.5756644812497225}, "step": 117}
Step:117	{"loss": {"pretrain_validation_loss": 6.78448420100742}, "step": 117}
Step:118	{"loss": {"learning_rate": 0.00022103975346886144}, "step": 118}
Step:118	{"loss": {"pretrain_training_loss": 2.568351546200839}, "step": 118}
Step:118	{"loss": {"pretrain_validation_loss": 6.419406519995795}, "step": 118}
Step:119	{"loss": {"learning_rate": 0.0002419941480818638}, "step": 119}
Step:119	{"loss": {"pretrain_training_loss": 2.5757061698219994}, "step": 119}
Step:119	{"loss": {"pretrain_validation_loss": 6.831486887402004}, "step": 119}
Step:0	{"loss": {"learning_rate": 0.003}, "step": 0}
Step:0	{"loss": {"pretrain_training_loss": 5.025560220082601}, "step": 0}
Step:0	{"loss": {"pretrain_validation_loss": 11.553745905558268}, "step": 0}
Step:1	{"loss": {"learning_rate": 0.0029994859874633356}, "step": 1}
Step:1	{"loss": {"pretrain_training_loss": 4.608770979775323}, "step": 1}
Step:1	{"loss": {"pretrain_validation_loss": 11.238502343495687}, "step": 1}
Step:2	{"loss": {"learning_rate": 0.0029979443021318605}, "step": 2}
Step:2	{"loss": {"pretrain_training_loss": 4.3712011443244085}, "step": 2}
Step:2	{"loss": {"pretrain_validation_loss": 12.197150548299154}, "step": 2}
Step:3	{"loss": {"learning_rate": 0.0029953760005996923}, "step": 3}
Step:3	{"loss": {"pretrain_training_loss": 4.258204009797838}, "step": 3}
Step:3	{"loss": {"pretrain_validation_loss": 7.8130176067352295}, "step": 3}
Step:4	{"loss": {"learning_rate": 0.00299178284305241}, "step": 4}
Step:4	{"loss": {"pretrain_training_loss": 4.194207880232069}, "step": 4}
Step:4	{"loss": {"pretrain_validation_loss": 7.897468646367391}, "step": 4}
Step:5	{"loss": {"learning_rate": 0.002987167292060716}, "step": 5}
Step:5	{"loss": {"pretrain_training_loss": 4.116921424865723}, "step": 5}
Step:5	{"loss": {"pretrain_validation_loss": 6.687256097793579}, "step": 5}
Step:6	{"loss": {"learning_rate": 0.002981532510892707}, "step": 6}
Step:6	{"loss": {"pretrain_training_loss": 4.084107769860162}, "step": 6}
Step:6	{"loss": {"pretrain_validation_loss": 8.374250888824463}, "step": 6}
Step:7	{"loss": {"learning_rate": 0.002974882361345932}, "step": 7}
Step:7	{"loss": {"pretrain_training_loss": 4.060323238372803}, "step": 7}
Step:7	{"loss": {"pretrain_validation_loss": 6.5249418417612715}, "step": 7}
Step:8	{"loss": {"learning_rate": 0.0029672214011007086}, "step": 8}
Step:8	{"loss": {"pretrain_training_loss": 3.9714406463834973}, "step": 8}
Step:8	{"loss": {"pretrain_validation_loss": 8.205545663833618}, "step": 8}
Step:9	{"loss": {"learning_rate": 0.002958554880596515}, "step": 9}
Step:9	{"loss": {"pretrain_training_loss": 3.845851037237379}, "step": 9}
Step:9	{"loss": {"pretrain_validation_loss": 8.366501967112223}, "step": 9}
Step:10	{"loss": {"learning_rate": 0.0029488887394336022}, "step": 10}
Step:10	{"loss": {"pretrain_training_loss": 3.7220709058973522}, "step": 10}
Step:10	{"loss": {"pretrain_validation_loss": 8.926940441131592}, "step": 10}
Step:11	{"loss": {"learning_rate": 0.0029382296023022893}, "step": 11}
Step:11	{"loss": {"pretrain_training_loss": 3.6052062114079795}, "step": 11}
Step:11	{"loss": {"pretrain_validation_loss": 8.823057651519775}, "step": 11}
Step:12	{"loss": {"learning_rate": 0.0029265847744427303}, "step": 12}
Step:12	{"loss": {"pretrain_training_loss": 3.4873216019736395}, "step": 12}
Step:12	{"loss": {"pretrain_validation_loss": 8.780938148498535}, "step": 12}
Step:13	{"loss": {"learning_rate": 0.0029139622366382676}, "step": 13}
Step:13	{"loss": {"pretrain_training_loss": 3.3186044295628867}, "step": 13}
Step:13	{"loss": {"pretrain_validation_loss": 7.51767102877299}, "step": 13}
Step:14	{"loss": {"learning_rate": 0.002900370639745802}, "step": 14}
Step:14	{"loss": {"pretrain_training_loss": 3.21691271993849}, "step": 14}
Step:14	{"loss": {"pretrain_validation_loss": 7.166167497634888}, "step": 14}
Step:15	{"loss": {"learning_rate": 0.0028858192987669296}, "step": 15}
Step:15	{"loss": {"pretrain_training_loss": 3.0787830352783203}, "step": 15}
Step:15	{"loss": {"pretrain_validation_loss": 5.482146422068278}, "step": 15}
Step:16	{"loss": {"learning_rate": 0.0028703181864639005}, "step": 16}
Step:16	{"loss": {"pretrain_training_loss": 3.0552549494637384}, "step": 16}
Step:16	{"loss": {"pretrain_validation_loss": 8.427916844685873}, "step": 16}
Step:17	{"loss": {"learning_rate": 0.0028538779265247904}, "step": 17}
Step:17	{"loss": {"pretrain_training_loss": 2.96951957543691}, "step": 17}
Step:17	{"loss": {"pretrain_validation_loss": 5.561899900436401}, "step": 17}
Step:18	{"loss": {"learning_rate": 0.002836509786282551}, "step": 18}
Step:18	{"loss": {"pretrain_training_loss": 2.7421615388658314}, "step": 18}
Step:18	{"loss": {"pretrain_validation_loss": 5.881604353586833}, "step": 18}
Step:19	{"loss": {"learning_rate": 0.002818225668992947}, "step": 19}
Step:19	{"loss": {"pretrain_training_loss": 2.6288203133477106}, "step": 19}
Step:19	{"loss": {"pretrain_validation_loss": 6.558260599772136}, "step": 19}
Step:20	{"loss": {"learning_rate": 0.0027990381056766573}, "step": 20}
Step:20	{"loss": {"pretrain_training_loss": 2.531439264615377}, "step": 20}
Step:20	{"loss": {"pretrain_validation_loss": 6.755150715510051}, "step": 20}
Step:21	{"loss": {"learning_rate": 0.002778960246531137}, "step": 21}
Step:21	{"loss": {"pretrain_training_loss": 2.4813312821918063}, "step": 21}
Step:21	{"loss": {"pretrain_validation_loss": 5.8070723215738935}, "step": 21}
Step:22	{"loss": {"learning_rate": 0.002758005851918135}, "step": 22}
Step:22	{"loss": {"pretrain_training_loss": 2.4807840320799084}, "step": 22}
Step:22	{"loss": {"pretrain_validation_loss": 5.8556333382924395}, "step": 22}
Step:23	{"loss": {"learning_rate": 0.0027361892829330225}, "step": 23}
Step:23	{"loss": {"pretrain_training_loss": 2.4376942449145846}, "step": 23}
Step:23	{"loss": {"pretrain_validation_loss": 4.89244286219279}, "step": 23}
Step:24	{"loss": {"learning_rate": 0.00271352549156242}, "step": 24}
Step:24	{"loss": {"pretrain_training_loss": 2.4392508533265858}, "step": 24}
Step:24	{"loss": {"pretrain_validation_loss": 4.809536457061768}, "step": 24}
Step:25	{"loss": {"learning_rate": 0.002690030010436852}, "step": 25}
Step:25	{"loss": {"pretrain_training_loss": 2.432552377382914}, "step": 25}
Step:25	{"loss": {"pretrain_validation_loss": 5.052412827809651}, "step": 25}
Step:26	{"loss": {"learning_rate": 0.0026657189421854556}, "step": 26}
Step:26	{"loss": {"pretrain_training_loss": 2.3375330898496838}, "step": 26}
Step:26	{"loss": {"pretrain_validation_loss": 4.8428317705790205}, "step": 26}
Step:27	{"loss": {"learning_rate": 0.0026406089484000456}, "step": 27}
Step:27	{"loss": {"pretrain_training_loss": 2.348683582411872}, "step": 27}
Step:27	{"loss": {"pretrain_validation_loss": 4.714582999547322}, "step": 27}
Step:28	{"loss": {"learning_rate": 0.0026147172382160904}, "step": 28}
Step:28	{"loss": {"pretrain_training_loss": 2.3319462405310736}, "step": 28}
Step:28	{"loss": {"pretrain_validation_loss": 5.344972531000773}, "step": 28}
Step:29	{"loss": {"learning_rate": 0.0025880615565184303}, "step": 29}
Step:29	{"loss": {"pretrain_training_loss": 2.29903072781033}, "step": 29}
Step:29	{"loss": {"pretrain_validation_loss": 5.010629177093506}, "step": 29}
Step:30	{"loss": {"learning_rate": 0.0025606601717798206}, "step": 30}
Step:30	{"loss": {"pretrain_training_loss": 2.2719001240200467}, "step": 30}
Step:30	{"loss": {"pretrain_validation_loss": 4.192987442016602}, "step": 30}
Step:31	{"loss": {"learning_rate": 0.00253253186354063}, "step": 31}
Step:31	{"loss": {"pretrain_training_loss": 2.2436456415388317}, "step": 31}
Step:31	{"loss": {"pretrain_validation_loss": 4.625167051951091}, "step": 31}
Step:32	{"loss": {"learning_rate": 0.002503695909538286}, "step": 32}
Step:32	{"loss": {"pretrain_training_loss": 2.2640801668167114}, "step": 32}
Step:32	{"loss": {"pretrain_validation_loss": 4.487050215403239}, "step": 32}
Step:33	{"loss": {"learning_rate": 0.0024741720724952743}, "step": 33}
Step:33	{"loss": {"pretrain_training_loss": 2.2266705168618097}, "step": 33}
Step:33	{"loss": {"pretrain_validation_loss": 4.456715663274129}, "step": 33}
Step:34	{"loss": {"learning_rate": 0.002443980586574755}, "step": 34}
Step:34	{"loss": {"pretrain_training_loss": 2.148234850830502}, "step": 34}
Step:34	{"loss": {"pretrain_validation_loss": 4.098208665847778}, "step": 34}
Step:35	{"loss": {"learning_rate": 0.00241314214351308}, "step": 35}
Step:35	{"loss": {"pretrain_training_loss": 2.142759700616201}, "step": 35}
Step:35	{"loss": {"pretrain_validation_loss": 4.255271911621094}, "step": 35}
Step:36	{"loss": {"learning_rate": 0.002381677878438709}, "step": 36}
Step:36	{"loss": {"pretrain_training_loss": 2.1378953324423895}, "step": 36}
Step:36	{"loss": {"pretrain_validation_loss": 5.088192860285441}, "step": 36}
Step:37	{"loss": {"learning_rate": 0.0023496093553872486}, "step": 37}
Step:37	{"loss": {"pretrain_training_loss": 2.140684598022037}, "step": 37}
Step:37	{"loss": {"pretrain_validation_loss": 4.9666233857472735}, "step": 37}
Step:38	{"loss": {"learning_rate": 0.00231695855252254}, "step": 38}
Step:38	{"loss": {"pretrain_training_loss": 2.1514180501302085}, "step": 38}
Step:38	{"loss": {"pretrain_validation_loss": 4.858023643493652}, "step": 38}
Step:39	{"loss": {"learning_rate": 0.0022837478470739223}, "step": 39}
Step:39	{"loss": {"pretrain_training_loss": 2.1290395657221475}, "step": 39}
Step:39	{"loss": {"pretrain_validation_loss": 4.184600353240967}, "step": 39}
Step:40	{"loss": {"learning_rate": 0.002249999999999999}, "step": 40}
Step:40	{"loss": {"pretrain_training_loss": 2.0671829183896384}, "step": 40}
Step:40	{"loss": {"pretrain_validation_loss": 4.333076238632202}, "step": 40}
Step:41	{"loss": {"learning_rate": 0.002215738140389412}, "step": 41}
Step:41	{"loss": {"pretrain_training_loss": 2.0780193408330283}, "step": 41}
Step:41	{"loss": {"pretrain_validation_loss": 5.608556588490804}, "step": 41}
Step:42	{"loss": {"learning_rate": 0.0021809857496093194}, "step": 42}
Step:42	{"loss": {"pretrain_training_loss": 2.069428112771776}, "step": 42}
Step:42	{"loss": {"pretrain_validation_loss": 5.84977658589681}, "step": 42}
Step:43	{"loss": {"learning_rate": 0.002145766645212442}, "step": 43}
Step:43	{"loss": {"pretrain_training_loss": 2.0327120555771723}, "step": 43}
Step:43	{"loss": {"pretrain_validation_loss": 4.340074857076009}, "step": 43}
Step:44	{"loss": {"learning_rate": 0.0021101049646136995}, "step": 44}
Step:44	{"loss": {"pretrain_training_loss": 1.9985468122694228}, "step": 44}
Step:44	{"loss": {"pretrain_validation_loss": 4.581770896911621}, "step": 44}
Step:45	{"loss": {"learning_rate": 0.0020740251485476336}, "step": 45}
Step:45	{"loss": {"pretrain_training_loss": 1.9697776436805725}, "step": 45}
Step:45	{"loss": {"pretrain_validation_loss": 4.020056327184041}, "step": 45}
Step:46	{"loss": {"learning_rate": 0.0020375519243179497}, "step": 46}
Step:46	{"loss": {"pretrain_training_loss": 1.9953497383329604}, "step": 46}
Step:46	{"loss": {"pretrain_validation_loss": 6.101194302241008}, "step": 46}
Step:47	{"loss": {"learning_rate": 0.0020007102888506554}, "step": 47}
Step:47	{"loss": {"pretrain_training_loss": 1.997219443321228}, "step": 47}
Step:47	{"loss": {"pretrain_validation_loss": 5.197387059529622}, "step": 47}
Step:48	{"loss": {"learning_rate": 0.0019635254915624204}, "step": 48}
Step:48	{"loss": {"pretrain_training_loss": 1.9769375059339735}, "step": 48}
Step:48	{"loss": {"pretrain_validation_loss": 5.1361111005147295}, "step": 48}
Step:49	{"loss": {"learning_rate": 0.0019260230170558834}, "step": 49}
Step:49	{"loss": {"pretrain_training_loss": 1.9258041381835938}, "step": 49}
Step:49	{"loss": {"pretrain_validation_loss": 4.9461822509765625}, "step": 49}
Step:50	{"loss": {"learning_rate": 0.0018882285676537801}, "step": 50}
Step:50	{"loss": {"pretrain_training_loss": 1.9520049492518108}, "step": 50}
Step:50	{"loss": {"pretrain_validation_loss": 4.909157037734985}, "step": 50}
Step:51	{"loss": {"learning_rate": 0.0018501680457838573}, "step": 51}
Step:51	{"loss": {"pretrain_training_loss": 1.9019543396102057}, "step": 51}
Step:51	{"loss": {"pretrain_validation_loss": 5.022975365320842}, "step": 51}
Step:52	{"loss": {"learning_rate": 0.0018118675362266377}, "step": 52}
Step:52	{"loss": {"pretrain_training_loss": 1.875654571586185}, "step": 52}
Step:52	{"loss": {"pretrain_validation_loss": 4.972917477289836}, "step": 52}
Step:53	{"loss": {"learning_rate": 0.0017733532882382203}, "step": 53}
Step:53	{"loss": {"pretrain_training_loss": 1.92268172899882}, "step": 53}
Step:53	{"loss": {"pretrain_validation_loss": 5.193412224451701}, "step": 53}
Step:54	{"loss": {"learning_rate": 0.0017346516975603455}, "step": 54}
Step:54	{"loss": {"pretrain_training_loss": 1.8707960181766086}, "step": 54}
Step:54	{"loss": {"pretrain_validation_loss": 4.539072275161743}, "step": 54}
Step:55	{"loss": {"learning_rate": 0.0016957892883300765}, "step": 55}
Step:55	{"loss": {"pretrain_training_loss": 1.879035234451294}, "step": 55}
Step:55	{"loss": {"pretrain_validation_loss": 5.245643615722656}, "step": 55}
Step:56	{"loss": {"learning_rate": 0.0016567926949014795}, "step": 56}
Step:56	{"loss": {"pretrain_training_loss": 1.8854179581006367}, "step": 56}
Step:56	{"loss": {"pretrain_validation_loss": 5.035117944081624}, "step": 56}
Step:57	{"loss": {"learning_rate": 0.0016176886435917668}, "step": 57}
Step:57	{"loss": {"pretrain_training_loss": 1.8797902994685702}, "step": 57}
Step:57	{"loss": {"pretrain_validation_loss": 4.808874686559041}, "step": 57}
Step:58	{"loss": {"learning_rate": 0.0015785039343644148}, "step": 58}
Step:58	{"loss": {"pretrain_training_loss": 1.8634158704015944}, "step": 58}
Step:58	{"loss": {"pretrain_validation_loss": 4.79798158009847}, "step": 58}
Step:59	{"loss": {"learning_rate": 0.0015392654224618088}, "step": 59}
Step:59	{"loss": {"pretrain_training_loss": 1.8658363686667547}, "step": 59}
Step:59	{"loss": {"pretrain_validation_loss": 4.549255768458049}, "step": 59}
Step:60	{"loss": {"learning_rate": 0.0014999999999999994}, "step": 60}
Step:60	{"loss": {"pretrain_training_loss": 1.825744138823615}, "step": 60}
Step:60	{"loss": {"pretrain_validation_loss": 4.565725723902385}, "step": 60}
Step:61	{"loss": {"learning_rate": 0.0014607345775381896}, "step": 61}
Step:61	{"loss": {"pretrain_training_loss": 1.8576523198021784}, "step": 61}
Step:61	{"loss": {"pretrain_validation_loss": 4.396405299504598}, "step": 61}
Step:62	{"loss": {"learning_rate": 0.0014214960656355837}, "step": 62}
Step:62	{"loss": {"pretrain_training_loss": 1.8511982427703009}, "step": 62}
Step:62	{"loss": {"pretrain_validation_loss": 4.690030415852864}, "step": 62}
Step:63	{"loss": {"learning_rate": 0.001382311356408232}, "step": 63}
Step:63	{"loss": {"pretrain_training_loss": 1.7897669209374323}, "step": 63}
Step:63	{"loss": {"pretrain_validation_loss": 4.638404210408528}, "step": 63}
Step:64	{"loss": {"learning_rate": 0.0013432073050985193}, "step": 64}
Step:64	{"loss": {"pretrain_training_loss": 1.840311348438263}, "step": 64}
Step:64	{"loss": {"pretrain_validation_loss": 4.5780729452768965}, "step": 64}
Step:65	{"loss": {"learning_rate": 0.0013042107116699223}, "step": 65}
Step:65	{"loss": {"pretrain_training_loss": 1.8209617469045851}, "step": 65}
Step:65	{"loss": {"pretrain_validation_loss": 4.481726169586182}, "step": 65}
Step:66	{"loss": {"learning_rate": 0.001265348302439653}, "step": 66}
Step:66	{"loss": {"pretrain_training_loss": 1.826420545578003}, "step": 66}
Step:66	{"loss": {"pretrain_validation_loss": 4.86315393447876}, "step": 66}
Step:67	{"loss": {"learning_rate": 0.0012266467117617785}, "step": 67}
Step:67	{"loss": {"pretrain_training_loss": 1.789012028111352}, "step": 67}
Step:67	{"loss": {"pretrain_validation_loss": 4.263548294703166}, "step": 67}
Step:68	{"loss": {"learning_rate": 0.0011881324637733604}, "step": 68}
Step:68	{"loss": {"pretrain_training_loss": 1.8089958826700847}, "step": 68}
Step:68	{"loss": {"pretrain_validation_loss": 4.421391646067302}, "step": 68}
Step:69	{"loss": {"learning_rate": 0.0011498319542161415}, "step": 69}
Step:69	{"loss": {"pretrain_training_loss": 1.7820460730128818}, "step": 69}
Step:69	{"loss": {"pretrain_validation_loss": 4.559284130732219}, "step": 69}
Step:70	{"loss": {"learning_rate": 0.0011117714323462186}, "step": 70}
Step:70	{"loss": {"pretrain_training_loss": 1.785926169819302}, "step": 70}
Step:70	{"loss": {"pretrain_validation_loss": 4.821345806121826}, "step": 70}
Step:71	{"loss": {"learning_rate": 0.0010739769829441156}, "step": 71}
Step:71	{"loss": {"pretrain_training_loss": 1.792178001668718}, "step": 71}
Step:71	{"loss": {"pretrain_validation_loss": 4.794512112935384}, "step": 71}
Step:72	{"loss": {"learning_rate": 0.0010364745084375786}, "step": 72}
Step:72	{"loss": {"pretrain_training_loss": 1.7668242785665724}, "step": 72}
Step:72	{"loss": {"pretrain_validation_loss": 4.889319817225139}, "step": 72}
Step:73	{"loss": {"learning_rate": 0.0009992897111493434}, "step": 73}
Step:73	{"loss": {"pretrain_training_loss": 1.7901370061768427}, "step": 73}
Step:73	{"loss": {"pretrain_validation_loss": 5.1353676319122314}, "step": 73}
Step:74	{"loss": {"learning_rate": 0.0009624480756820492}, "step": 74}
Step:74	{"loss": {"pretrain_training_loss": 1.7593354384104412}, "step": 74}
Step:74	{"loss": {"pretrain_validation_loss": 4.601704756418864}, "step": 74}
Step:75	{"loss": {"learning_rate": 0.000925974851452365}, "step": 75}
Step:75	{"loss": {"pretrain_training_loss": 1.7696120805210538}, "step": 75}
Step:75	{"loss": {"pretrain_validation_loss": 5.003632704416911}, "step": 75}
Step:76	{"loss": {"learning_rate": 0.0008898950353862994}, "step": 76}
Step:76	{"loss": {"pretrain_training_loss": 1.7341819008191426}, "step": 76}
Step:76	{"loss": {"pretrain_validation_loss": 4.809452533721924}, "step": 76}
Step:77	{"loss": {"learning_rate": 0.0008542333547875573}, "step": 77}
Step:77	{"loss": {"pretrain_training_loss": 1.7416952782207065}, "step": 77}
Step:77	{"loss": {"pretrain_validation_loss": 4.859244028727214}, "step": 77}
Step:78	{"loss": {"learning_rate": 0.0008190142503906795}, "step": 78}
Step:78	{"loss": {"pretrain_training_loss": 1.7266075147522821}, "step": 78}
Step:78	{"loss": {"pretrain_validation_loss": 5.01570709546407}, "step": 78}
Step:79	{"loss": {"learning_rate": 0.000784261859610587}, "step": 79}
Step:79	{"loss": {"pretrain_training_loss": 1.7475466198391385}, "step": 79}
Step:79	{"loss": {"pretrain_validation_loss": 4.591481844584147}, "step": 79}
Step:80	{"loss": {"learning_rate": 0.00075}, "step": 80}
Step:80	{"loss": {"pretrain_training_loss": 1.724985698858897}, "step": 80}
Step:80	{"loss": {"pretrain_validation_loss": 5.021954456965129}, "step": 80}
Step:81	{"loss": {"learning_rate": 0.0007162521529260764}, "step": 81}
Step:81	{"loss": {"pretrain_training_loss": 1.7077329225010343}, "step": 81}
Step:81	{"loss": {"pretrain_validation_loss": 4.799369255701701}, "step": 81}
Step:82	{"loss": {"learning_rate": 0.0006830414474774595}, "step": 82}
Step:82	{"loss": {"pretrain_training_loss": 1.7564537988768683}, "step": 82}
Step:82	{"loss": {"pretrain_validation_loss": 4.900889158248901}, "step": 82}
Step:83	{"loss": {"learning_rate": 0.00065039064461275}, "step": 83}
Step:83	{"loss": {"pretrain_training_loss": 1.7354663742913141}, "step": 83}
Step:83	{"loss": {"pretrain_validation_loss": 5.075849374135335}, "step": 83}
Step:84	{"loss": {"learning_rate": 0.0006183221215612901}, "step": 84}
Step:84	{"loss": {"pretrain_training_loss": 1.7251968648698595}, "step": 84}
Step:84	{"loss": {"pretrain_validation_loss": 4.665738344192505}, "step": 84}
Step:85	{"loss": {"learning_rate": 0.0005868578564869187}, "step": 85}
Step:85	{"loss": {"pretrain_training_loss": 1.7002438373035855}, "step": 85}
Step:85	{"loss": {"pretrain_validation_loss": 4.637487093607585}, "step": 85}
Step:86	{"loss": {"learning_rate": 0.0005560194134252438}, "step": 86}
Step:86	{"loss": {"pretrain_training_loss": 1.7397863931126065}, "step": 86}
Step:86	{"loss": {"pretrain_validation_loss": 4.632037242253621}, "step": 86}
Step:87	{"loss": {"learning_rate": 0.0005258279275047245}, "step": 87}
Step:87	{"loss": {"pretrain_training_loss": 1.7205133438110352}, "step": 87}
Step:87	{"loss": {"pretrain_validation_loss": 4.708252350489299}, "step": 87}
Step:88	{"loss": {"learning_rate": 0.0004963040904617129}, "step": 88}
Step:88	{"loss": {"pretrain_training_loss": 1.757143219312032}, "step": 88}
Step:88	{"loss": {"pretrain_validation_loss": 4.581808725992839}, "step": 88}
Step:89	{"loss": {"learning_rate": 0.00046746813645936856}, "step": 89}
Step:89	{"loss": {"pretrain_training_loss": 1.7153490516874526}, "step": 89}
Step:89	{"loss": {"pretrain_validation_loss": 4.753641923268636}, "step": 89}
Step:90	{"loss": {"learning_rate": 0.00043933982822017855}, "step": 90}
Step:90	{"loss": {"pretrain_training_loss": 1.6799171103371515}, "step": 90}
Step:90	{"loss": {"pretrain_validation_loss": 4.723980108896892}, "step": 90}
Step:91	{"loss": {"learning_rate": 0.0004119384434815683}, "step": 91}
Step:91	{"loss": {"pretrain_training_loss": 1.691939155260722}, "step": 91}
Step:91	{"loss": {"pretrain_validation_loss": 4.656339565912883}, "step": 91}
Step:92	{"loss": {"learning_rate": 0.0003852827617839086}, "step": 92}
Step:92	{"loss": {"pretrain_training_loss": 1.6765319969918993}, "step": 92}
Step:92	{"loss": {"pretrain_validation_loss": 4.781778732935588}, "step": 92}
Step:93	{"loss": {"learning_rate": 0.00035939105159995345}, "step": 93}
Step:93	{"loss": {"pretrain_training_loss": 1.7236940794520907}, "step": 93}
Step:93	{"loss": {"pretrain_validation_loss": 4.934359073638916}, "step": 93}
Step:94	{"loss": {"learning_rate": 0.00033428105781454354}, "step": 94}
Step:94	{"loss": {"pretrain_training_loss": 1.6942501465479534}, "step": 94}
Step:94	{"loss": {"pretrain_validation_loss": 4.889302015304565}, "step": 94}
Step:95	{"loss": {"learning_rate": 0.000309969989563147}, "step": 95}
Step:95	{"loss": {"pretrain_training_loss": 1.6660611364576552}, "step": 95}
Step:95	{"loss": {"pretrain_validation_loss": 5.03288197517395}, "step": 95}
Step:96	{"loss": {"learning_rate": 0.00028647450843757887}, "step": 96}
Step:96	{"loss": {"pretrain_training_loss": 1.7157770064142015}, "step": 96}
Step:96	{"loss": {"pretrain_validation_loss": 4.984663804372151}, "step": 96}
Step:97	{"loss": {"learning_rate": 0.00026381071706697666}, "step": 97}
Step:97	{"loss": {"pretrain_training_loss": 1.7146648698382907}, "step": 97}
Step:97	{"loss": {"pretrain_validation_loss": 4.80903434753418}, "step": 97}
Step:98	{"loss": {"learning_rate": 0.00024199414808186398}, "step": 98}
Step:98	{"loss": {"pretrain_training_loss": 1.6670585009786818}, "step": 98}
Step:98	{"loss": {"pretrain_validation_loss": 5.051925818125407}, "step": 98}
Step:99	{"loss": {"learning_rate": 0.00022103975346886163}, "step": 99}
Step:99	{"loss": {"pretrain_training_loss": 1.6996784607569377}, "step": 99}
Step:99	{"loss": {"pretrain_validation_loss": 4.815808216730754}, "step": 99}
Step:100	{"loss": {"learning_rate": 0.00020096189432334185}, "step": 100}
Step:100	{"loss": {"pretrain_training_loss": 1.6795550915930006}, "step": 100}
Step:100	{"loss": {"pretrain_validation_loss": 4.771776517232259}, "step": 100}
Step:101	{"loss": {"learning_rate": 0.000181774331007052}, "step": 101}
Step:101	{"loss": {"pretrain_training_loss": 1.6576285362243652}, "step": 101}
Step:101	{"loss": {"pretrain_validation_loss": 4.72626789410909}, "step": 101}
Step:102	{"loss": {"learning_rate": 0.00016349021371744825}, "step": 102}
Step:102	{"loss": {"pretrain_training_loss": 1.7019037405649822}, "step": 102}
Step:102	{"loss": {"pretrain_validation_loss": 4.785388867060344}, "step": 102}
Step:103	{"loss": {"learning_rate": 0.00014612207347520934}, "step": 103}
Step:103	{"loss": {"pretrain_training_loss": 1.6593596537907918}, "step": 103}
Step:103	{"loss": {"pretrain_validation_loss": 4.815219004948934}, "step": 103}
Step:104	{"loss": {"learning_rate": 0.0001296818135360985}, "step": 104}
Step:104	{"loss": {"pretrain_training_loss": 1.694451093673706}, "step": 104}
Step:104	{"loss": {"pretrain_validation_loss": 4.751758098602295}, "step": 104}
Step:105	{"loss": {"learning_rate": 0.00011418070123306987}, "step": 105}
Step:105	{"loss": {"pretrain_training_loss": 1.6420759757359822}, "step": 105}
Step:105	{"loss": {"pretrain_validation_loss": 5.036647717158}, "step": 105}
Step:106	{"loss": {"learning_rate": 9.962936025419735e-05}, "step": 106}
Step:106	{"loss": {"pretrain_training_loss": 1.683476275867886}, "step": 106}
Step:106	{"loss": {"pretrain_validation_loss": 4.900335709253947}, "step": 106}
Step:107	{"loss": {"learning_rate": 8.60377633617325e-05}, "step": 107}
Step:107	{"loss": {"pretrain_training_loss": 1.6701494455337524}, "step": 107}
Step:107	{"loss": {"pretrain_validation_loss": 4.973726828893025}, "step": 107}
Step:108	{"loss": {"learning_rate": 7.341522555726968e-05}, "step": 108}
Step:108	{"loss": {"pretrain_training_loss": 1.7050419516033597}, "step": 108}
Step:108	{"loss": {"pretrain_validation_loss": 4.775791486104329}, "step": 108}
Step:109	{"loss": {"learning_rate": 6.177039769771057e-05}, "step": 109}
Step:109	{"loss": {"pretrain_training_loss": 1.6764740016725328}, "step": 109}
Step:109	{"loss": {"pretrain_validation_loss": 4.969638109207153}, "step": 109}
Step:110	{"loss": {"learning_rate": 5.111126056639751e-05}, "step": 110}
Step:110	{"loss": {"pretrain_training_loss": 1.6903085576163397}, "step": 110}
Step:110	{"loss": {"pretrain_validation_loss": 5.0186835924784345}, "step": 110}
Step:111	{"loss": {"learning_rate": 4.144511940348515e-05}, "step": 111}
Step:111	{"loss": {"pretrain_training_loss": 1.6870883107185364}, "step": 111}
Step:111	{"loss": {"pretrain_validation_loss": 5.059672911961873}, "step": 111}
Step:112	{"loss": {"learning_rate": 3.277859889929146e-05}, "step": 112}
Step:112	{"loss": {"pretrain_training_loss": 1.6740117006831698}, "step": 112}
Step:112	{"loss": {"pretrain_validation_loss": 5.022544940312703}, "step": 112}
Step:113	{"loss": {"learning_rate": 2.5117638654068232e-05}, "step": 113}
Step:113	{"loss": {"pretrain_training_loss": 1.698046346505483}, "step": 113}
Step:113	{"loss": {"pretrain_validation_loss": 4.8362390995025635}, "step": 113}
Step:114	{"loss": {"learning_rate": 1.8467489107293506e-05}, "step": 114}
Step:114	{"loss": {"pretrain_training_loss": 1.6780971354908414}, "step": 114}
Step:114	{"loss": {"pretrain_validation_loss": 4.8984324137369795}, "step": 114}
Step:115	{"loss": {"learning_rate": 1.2832707939284424e-05}, "step": 115}
Step:115	{"loss": {"pretrain_training_loss": 1.6716483301586575}, "step": 115}
Step:115	{"loss": {"pretrain_validation_loss": 4.89735221862793}, "step": 115}
Step:116	{"loss": {"learning_rate": 8.217156947589898e-06}, "step": 116}
Step:116	{"loss": {"pretrain_training_loss": 1.6612995200686984}, "step": 116}
Step:116	{"loss": {"pretrain_validation_loss": 4.965223073959351}, "step": 116}
Step:117	{"loss": {"learning_rate": 4.623999400308054e-06}, "step": 117}
Step:117	{"loss": {"pretrain_training_loss": 1.6766551004515753}, "step": 117}
Step:117	{"loss": {"pretrain_validation_loss": 4.96231206258138}, "step": 117}
Step:118	{"loss": {"learning_rate": 2.05569786813925e-06}, "step": 118}
Step:118	{"loss": {"pretrain_training_loss": 1.6812226904763117}, "step": 118}
Step:118	{"loss": {"pretrain_validation_loss": 4.859335104624431}, "step": 118}
Step:119	{"loss": {"learning_rate": 5.140125366641102e-07}, "step": 119}
Step:119	{"loss": {"pretrain_training_loss": 1.6723185910118952}, "step": 119}
Step:119	{"loss": {"pretrain_validation_loss": 4.823205471038818}, "step": 119}
Step:0	{"loss": {"learning_rate": 0.003}, "step": 0}
Step:0	{"loss": {"pretrain_training_loss": 3.1656601092186114}, "step": 0}
Step:0	{"loss": {"pretrain_validation_loss": 3.506362883940987}, "step": 0}
Step:1	{"loss": {"learning_rate": 0.0029994859874633356}, "step": 1}
Step:1	{"loss": {"pretrain_training_loss": 2.0213096496340954}, "step": 1}
Step:1	{"loss": {"pretrain_validation_loss": 2.8508532617403115}, "step": 1}
Step:2	{"loss": {"learning_rate": 0.0029979443021318605}, "step": 2}
Step:2	{"loss": {"pretrain_training_loss": 1.4177951154655684}, "step": 2}
Step:2	{"loss": {"pretrain_validation_loss": 2.5812090203381968}, "step": 2}
Step:3	{"loss": {"learning_rate": 0.0029953760005996923}, "step": 3}
Step:3	{"loss": {"pretrain_training_loss": 1.0664081828301724}, "step": 3}
Step:3	{"loss": {"pretrain_validation_loss": 2.2115268309911094}, "step": 3}
Step:4	{"loss": {"learning_rate": 0.00299178284305241}, "step": 4}
Step:4	{"loss": {"pretrain_training_loss": 0.8487283438998084}, "step": 4}
Step:4	{"loss": {"pretrain_validation_loss": 2.1086829918018286}, "step": 4}
Step:5	{"loss": {"learning_rate": 0.002987167292060716}, "step": 5}
Step:5	{"loss": {"pretrain_training_loss": 0.7021220604504794}, "step": 5}
Step:5	{"loss": {"pretrain_validation_loss": 1.9961793509082517}, "step": 5}
Step:6	{"loss": {"learning_rate": 0.002981532510892707}, "step": 6}
Step:6	{"loss": {"pretrain_training_loss": 0.6297753664862268}, "step": 6}
Step:6	{"loss": {"pretrain_validation_loss": 1.8430412679478743}, "step": 6}
Step:7	{"loss": {"learning_rate": 0.002974882361345932}, "step": 7}
Step:7	{"loss": {"pretrain_training_loss": 0.5869771287565337}, "step": 7}
Step:7	{"loss": {"pretrain_validation_loss": 1.8311170089072075}, "step": 7}
Step:8	{"loss": {"learning_rate": 0.0029672214011007086}, "step": 8}
Step:8	{"loss": {"pretrain_training_loss": 0.5821139183186244}, "step": 8}
Step:8	{"loss": {"pretrain_validation_loss": 1.8747663817543914}, "step": 8}
Step:9	{"loss": {"learning_rate": 0.002958554880596515}, "step": 9}
Step:9	{"loss": {"pretrain_training_loss": 0.5119932650633461}, "step": 9}
Step:9	{"loss": {"pretrain_validation_loss": 1.766509487145189}, "step": 9}
Step:10	{"loss": {"learning_rate": 0.0029488887394336022}, "step": 10}
Step:10	{"loss": {"pretrain_training_loss": 0.4868675921264634}, "step": 10}
Step:10	{"loss": {"pretrain_validation_loss": 1.814817992673404}, "step": 10}
Step:11	{"loss": {"learning_rate": 0.0029382296023022893}, "step": 11}
Step:11	{"loss": {"pretrain_training_loss": 0.4789429318062878}, "step": 11}
Step:11	{"loss": {"pretrain_validation_loss": 1.728280120137809}, "step": 11}
Step:12	{"loss": {"learning_rate": 0.0029265847744427303}, "step": 12}
Step:12	{"loss": {"pretrain_training_loss": 0.45937159901214797}, "step": 12}
Step:12	{"loss": {"pretrain_validation_loss": 1.7682891453521838}, "step": 12}
Step:13	{"loss": {"learning_rate": 0.0029139622366382676}, "step": 13}
Step:13	{"loss": {"pretrain_training_loss": 0.432790759549265}, "step": 13}
Step:13	{"loss": {"pretrain_validation_loss": 1.793893453867539}, "step": 13}
Step:14	{"loss": {"learning_rate": 0.002900370639745802}, "step": 14}
Step:14	{"loss": {"pretrain_training_loss": 0.4291060360280111}, "step": 14}
Step:14	{"loss": {"pretrain_validation_loss": 1.6965254795724067}, "step": 14}
Step:15	{"loss": {"learning_rate": 0.0028858192987669296}, "step": 15}
Step:15	{"loss": {"pretrain_training_loss": 0.4097085370782583}, "step": 15}
Step:15	{"loss": {"pretrain_validation_loss": 1.7456046869789346}, "step": 15}
Step:16	{"loss": {"learning_rate": 0.0028703181864639005}, "step": 16}
Step:16	{"loss": {"pretrain_training_loss": 0.3995887898047174}, "step": 16}
Step:16	{"loss": {"pretrain_validation_loss": 1.9180322349935337}, "step": 16}
Step:17	{"loss": {"learning_rate": 0.0028538779265247904}, "step": 17}
Step:17	{"loss": {"pretrain_training_loss": 0.40499096854025546}, "step": 17}
Step:17	{"loss": {"pretrain_validation_loss": 1.8376205399416494}, "step": 17}
Step:18	{"loss": {"learning_rate": 0.002836509786282551}, "step": 18}
Step:18	{"loss": {"pretrain_training_loss": 0.39156380334529733}, "step": 18}
Step:18	{"loss": {"pretrain_validation_loss": 1.6966104637021604}, "step": 18}
Step:19	{"loss": {"learning_rate": 0.002818225668992947}, "step": 19}
Step:19	{"loss": {"pretrain_training_loss": 0.37818801630163723}, "step": 19}
Step:19	{"loss": {"pretrain_validation_loss": 1.6649877109389375}, "step": 19}
Step:20	{"loss": {"learning_rate": 0.0027990381056766573}, "step": 20}
Step:20	{"loss": {"pretrain_training_loss": 0.37667896656741884}, "step": 20}
Step:20	{"loss": {"pretrain_validation_loss": 1.596739143565081}, "step": 20}
Step:21	{"loss": {"learning_rate": 0.002778960246531137}, "step": 21}
Step:21	{"loss": {"pretrain_training_loss": 0.354496789078287}, "step": 21}
Step:21	{"loss": {"pretrain_validation_loss": 1.6775415341059368}, "step": 21}
Step:22	{"loss": {"learning_rate": 0.002758005851918135}, "step": 22}
Step:22	{"loss": {"pretrain_training_loss": 0.3626079226404318}, "step": 22}
Step:22	{"loss": {"pretrain_validation_loss": 1.5659922370012256}, "step": 22}
Step:23	{"loss": {"learning_rate": 0.0027361892829330225}, "step": 23}
Step:23	{"loss": {"pretrain_training_loss": 0.34678669146445606}, "step": 23}
Step:23	{"loss": {"pretrain_validation_loss": 1.5964228640431943}, "step": 23}
Step:24	{"loss": {"learning_rate": 0.00271352549156242}, "step": 24}
Step:24	{"loss": {"pretrain_training_loss": 0.3439732147969278}, "step": 24}
Step:24	{"loss": {"pretrain_validation_loss": 1.5708105296328447}, "step": 24}
Step:25	{"loss": {"learning_rate": 0.002690030010436852}, "step": 25}
Step:25	{"loss": {"pretrain_training_loss": 0.34286255126327386}, "step": 25}
Step:25	{"loss": {"pretrain_validation_loss": 1.585550189882085}, "step": 25}
Step:26	{"loss": {"learning_rate": 0.0026657189421854556}, "step": 26}
Step:26	{"loss": {"pretrain_training_loss": 0.3344180265988559}, "step": 26}
Step:26	{"loss": {"pretrain_validation_loss": 1.5964286396468894}, "step": 26}
Step:27	{"loss": {"learning_rate": 0.0026406089484000456}, "step": 27}
Step:27	{"loss": {"pretrain_training_loss": 0.3270963915661808}, "step": 27}
Step:27	{"loss": {"pretrain_validation_loss": 1.5681320230166118}, "step": 27}
Step:28	{"loss": {"learning_rate": 0.0026147172382160904}, "step": 28}
Step:28	{"loss": {"pretrain_training_loss": 0.32389693222524507}, "step": 28}
Step:28	{"loss": {"pretrain_validation_loss": 1.5400405595268027}, "step": 28}
Step:29	{"loss": {"learning_rate": 0.0025880615565184303}, "step": 29}
Step:29	{"loss": {"pretrain_training_loss": 0.3249545827452578}, "step": 29}
Step:29	{"loss": {"pretrain_validation_loss": 2.2377222880073218}, "step": 29}
Step:30	{"loss": {"learning_rate": 0.0025606601717798206}, "step": 30}
Step:30	{"loss": {"pretrain_training_loss": 0.32292839303114157}, "step": 30}
Step:30	{"loss": {"pretrain_validation_loss": 1.639540193737417}, "step": 30}
Step:31	{"loss": {"learning_rate": 0.00253253186354063}, "step": 31}
Step:31	{"loss": {"pretrain_training_loss": 0.3056357289667909}, "step": 31}
Step:31	{"loss": {"pretrain_validation_loss": 1.4808547125346418}, "step": 31}
Step:32	{"loss": {"learning_rate": 0.002503695909538286}, "step": 32}
Step:32	{"loss": {"pretrain_training_loss": 0.30591260085097033}, "step": 32}
Step:32	{"loss": {"pretrain_validation_loss": 1.663513491982999}, "step": 32}
Step:33	{"loss": {"learning_rate": 0.0024741720724952743}, "step": 33}
Step:33	{"loss": {"pretrain_training_loss": 0.2989091970109585}, "step": 33}
Step:33	{"loss": {"pretrain_validation_loss": 1.585668532744698}, "step": 33}
Step:34	{"loss": {"learning_rate": 0.002443980586574755}, "step": 34}
Step:34	{"loss": {"pretrain_training_loss": 0.2967222655351277}, "step": 34}
Step:34	{"loss": {"pretrain_validation_loss": 1.5368881882100864}, "step": 34}
Step:35	{"loss": {"learning_rate": 0.00241314214351308}, "step": 35}
Step:35	{"loss": {"pretrain_training_loss": 0.305861359358277}, "step": 35}
Step:35	{"loss": {"pretrain_validation_loss": 1.5849827096082163}, "step": 35}
Step:36	{"loss": {"learning_rate": 0.002381677878438709}, "step": 36}
Step:36	{"loss": {"pretrain_training_loss": 0.2858219282445411}, "step": 36}
Step:36	{"loss": {"pretrain_validation_loss": 1.4635752785033074}, "step": 36}
Step:37	{"loss": {"learning_rate": 0.0023496093553872486}, "step": 37}
Step:37	{"loss": {"pretrain_training_loss": 0.29179231013728785}, "step": 37}
Step:37	{"loss": {"pretrain_validation_loss": 1.5359354174655417}, "step": 37}
Step:38	{"loss": {"learning_rate": 0.00231695855252254}, "step": 38}
Step:38	{"loss": {"pretrain_training_loss": 0.2790186675507783}, "step": 38}
Step:38	{"loss": {"pretrain_validation_loss": 1.6296489601549895}, "step": 38}
Step:39	{"loss": {"learning_rate": 0.0022837478470739223}, "step": 39}
Step:39	{"loss": {"pretrain_training_loss": 0.2859011784362084}, "step": 39}
Step:39	{"loss": {"pretrain_validation_loss": 1.5951313886089602}, "step": 39}
Step:40	{"loss": {"learning_rate": 0.002249999999999999}, "step": 40}
Step:40	{"loss": {"pretrain_training_loss": 0.28630853336318274}, "step": 40}
Step:40	{"loss": {"pretrain_validation_loss": 1.640573943870655}, "step": 40}
Step:41	{"loss": {"learning_rate": 0.002215738140389412}, "step": 41}
Step:41	{"loss": {"pretrain_training_loss": 0.268170919283172}, "step": 41}
Step:41	{"loss": {"pretrain_validation_loss": 1.4821079356082971}, "step": 41}
Step:42	{"loss": {"learning_rate": 0.0021809857496093194}, "step": 42}
Step:42	{"loss": {"pretrain_training_loss": 0.26602214469564006}, "step": 42}
Step:42	{"loss": {"pretrain_validation_loss": 1.5430553166762642}, "step": 42}
Step:43	{"loss": {"learning_rate": 0.002145766645212442}, "step": 43}
Step:43	{"loss": {"pretrain_training_loss": 0.2737115435440744}, "step": 43}
Step:43	{"loss": {"pretrain_validation_loss": 1.573052249956822}, "step": 43}
Step:44	{"loss": {"learning_rate": 0.0021101049646136995}, "step": 44}
Step:44	{"loss": {"pretrain_training_loss": 0.26271350000427557}, "step": 44}
Step:44	{"loss": {"pretrain_validation_loss": 1.3434078373770784}, "step": 44}
Step:45	{"loss": {"learning_rate": 0.0020740251485476336}, "step": 45}
Step:45	{"loss": {"pretrain_training_loss": 0.2658185328471173}, "step": 45}
Step:45	{"loss": {"pretrain_validation_loss": 1.7989721280941064}, "step": 45}
Step:46	{"loss": {"learning_rate": 0.0020375519243179497}, "step": 46}
Step:46	{"loss": {"pretrain_training_loss": 0.2672375176918994}, "step": 46}
Step:46	{"loss": {"pretrain_validation_loss": 1.4555690901866858}, "step": 46}
Step:47	{"loss": {"learning_rate": 0.0020007102888506554}, "step": 47}
Step:47	{"loss": {"pretrain_training_loss": 0.2643893778213338}, "step": 47}
Step:47	{"loss": {"pretrain_validation_loss": 1.587817711242731}, "step": 47}
Step:48	{"loss": {"learning_rate": 0.0019635254915624204}, "step": 48}
Step:48	{"loss": {"pretrain_training_loss": 0.25729484568076505}, "step": 48}
Step:48	{"loss": {"pretrain_validation_loss": 1.5992370835248975}, "step": 48}
Step:49	{"loss": {"learning_rate": 0.0019260230170558834}, "step": 49}
Step:49	{"loss": {"pretrain_training_loss": 0.2543639805181762}, "step": 49}
Step:49	{"loss": {"pretrain_validation_loss": 1.5960660274477974}, "step": 49}
Step:50	{"loss": {"learning_rate": 0.0018882285676537801}, "step": 50}
Step:50	{"loss": {"pretrain_training_loss": 0.25262125473483343}, "step": 50}
Step:50	{"loss": {"pretrain_validation_loss": 1.397099645241447}, "step": 50}
Step:51	{"loss": {"learning_rate": 0.0018501680457838573}, "step": 51}
Step:51	{"loss": {"pretrain_training_loss": 0.2534667001171626}, "step": 51}
Step:51	{"loss": {"pretrain_validation_loss": 1.488753993442093}, "step": 51}
Step:52	{"loss": {"learning_rate": 0.0018118675362266377}, "step": 52}
Step:52	{"loss": {"pretrain_training_loss": 0.24506440645146105}, "step": 52}
Step:52	{"loss": {"pretrain_validation_loss": 1.4563328191853953}, "step": 52}
Step:53	{"loss": {"learning_rate": 0.0017733532882382203}, "step": 53}
Step:53	{"loss": {"pretrain_training_loss": 0.2448728550156253}, "step": 53}
Step:53	{"loss": {"pretrain_validation_loss": 1.3990531010904175}, "step": 53}
Step:54	{"loss": {"learning_rate": 0.0017346516975603455}, "step": 54}
Step:54	{"loss": {"pretrain_training_loss": 0.24101017108083214}, "step": 54}
Step:54	{"loss": {"pretrain_validation_loss": 1.4108330322348552}, "step": 54}
Step:55	{"loss": {"learning_rate": 0.0016957892883300765}, "step": 55}
Step:55	{"loss": {"pretrain_training_loss": 0.23858274275707048}, "step": 55}
Step:55	{"loss": {"pretrain_validation_loss": 1.3027847599292146}, "step": 55}
Step:56	{"loss": {"learning_rate": 0.0016567926949014795}, "step": 56}
Step:56	{"loss": {"pretrain_training_loss": 0.243158217671857}, "step": 56}
Step:56	{"loss": {"pretrain_validation_loss": 1.3582400999207427}, "step": 56}
Step:57	{"loss": {"learning_rate": 0.0016176886435917668}, "step": 57}
Step:57	{"loss": {"pretrain_training_loss": 0.23926431206522378}, "step": 57}
Step:57	{"loss": {"pretrain_validation_loss": 1.4946632678957954}, "step": 57}
Step:58	{"loss": {"learning_rate": 0.0015785039343644148}, "step": 58}
Step:58	{"loss": {"pretrain_training_loss": 0.23235725331705298}, "step": 58}
Step:58	{"loss": {"pretrain_validation_loss": 1.3550847431887751}, "step": 58}
Step:59	{"loss": {"learning_rate": 0.0015392654224618088}, "step": 59}
Step:59	{"loss": {"pretrain_training_loss": 0.23870551608308985}, "step": 59}
Step:59	{"loss": {"pretrain_validation_loss": 1.4367460090181101}, "step": 59}
Step:60	{"loss": {"learning_rate": 0.0014999999999999994}, "step": 60}
Step:60	{"loss": {"pretrain_training_loss": 0.23063658820430585}, "step": 60}
Step:60	{"loss": {"pretrain_validation_loss": 1.4493545699810637}, "step": 60}
Step:61	{"loss": {"learning_rate": 0.0014607345775381896}, "step": 61}
Step:61	{"loss": {"pretrain_training_loss": 0.23882619252767703}, "step": 61}
Step:61	{"loss": {"pretrain_validation_loss": 1.5014767318532087}, "step": 61}
Step:62	{"loss": {"learning_rate": 0.0014214960656355837}, "step": 62}
Step:62	{"loss": {"pretrain_training_loss": 0.23090545442578517}, "step": 62}
Step:62	{"loss": {"pretrain_validation_loss": 1.4005918839703435}, "step": 62}
Step:63	{"loss": {"learning_rate": 0.001382311356408232}, "step": 63}
Step:63	{"loss": {"pretrain_training_loss": 0.2214085886801929}, "step": 63}
Step:63	{"loss": {"pretrain_validation_loss": 1.4927974844324416}, "step": 63}
Step:64	{"loss": {"learning_rate": 0.0013432073050985193}, "step": 64}
Step:64	{"loss": {"pretrain_training_loss": 0.22406645022249577}, "step": 64}
Step:64	{"loss": {"pretrain_validation_loss": 1.410244265328283}, "step": 64}
Step:65	{"loss": {"learning_rate": 0.0013042107116699223}, "step": 65}
Step:65	{"loss": {"pretrain_training_loss": 0.215348591342513}, "step": 65}
Step:65	{"loss": {"pretrain_validation_loss": 1.3676872573037078}, "step": 65}
Step:66	{"loss": {"learning_rate": 0.001265348302439653}, "step": 66}
Step:66	{"loss": {"pretrain_training_loss": 0.22331534162662287}, "step": 66}
Step:66	{"loss": {"pretrain_validation_loss": 1.5019391507342241}, "step": 66}
Step:67	{"loss": {"learning_rate": 0.0012266467117617785}, "step": 67}
Step:67	{"loss": {"pretrain_training_loss": 0.22002141327219824}, "step": 67}
Step:67	{"loss": {"pretrain_validation_loss": 1.4049856965092644}, "step": 67}
Step:68	{"loss": {"learning_rate": 0.0011881324637733604}, "step": 68}
Step:68	{"loss": {"pretrain_training_loss": 0.22435422695923915}, "step": 68}
Step:68	{"loss": {"pretrain_validation_loss": 1.3707710133082625}, "step": 68}
Step:69	{"loss": {"learning_rate": 0.0011498319542161415}, "step": 69}
Step:69	{"loss": {"pretrain_training_loss": 0.2204527503163398}, "step": 69}
Step:69	{"loss": {"pretrain_validation_loss": 1.41558817849643}, "step": 69}
Step:70	{"loss": {"learning_rate": 0.0011117714323462186}, "step": 70}
Step:70	{"loss": {"pretrain_training_loss": 0.22088196408349786}, "step": 70}
Step:70	{"loss": {"pretrain_validation_loss": 1.3365454881087593}, "step": 70}
Step:71	{"loss": {"learning_rate": 0.0010739769829441156}, "step": 71}
Step:71	{"loss": {"pretrain_training_loss": 0.2200330251266965}, "step": 71}
Step:71	{"loss": {"pretrain_validation_loss": 1.4630832421606865}, "step": 71}
Step:72	{"loss": {"learning_rate": 0.0010364745084375786}, "step": 72}
Step:72	{"loss": {"pretrain_training_loss": 0.21167843783322762}, "step": 72}
Step:72	{"loss": {"pretrain_validation_loss": 1.396877938422604}, "step": 72}
Step:73	{"loss": {"learning_rate": 0.0009992897111493434}, "step": 73}
Step:73	{"loss": {"pretrain_training_loss": 0.21278420209773855}, "step": 73}
Step:73	{"loss": {"pretrain_validation_loss": 1.4792004296745078}, "step": 73}
Step:74	{"loss": {"learning_rate": 0.0009624480756820492}, "step": 74}
Step:74	{"loss": {"pretrain_training_loss": 0.21212717840884254}, "step": 74}
Step:74	{"loss": {"pretrain_validation_loss": 1.3705176201419553}, "step": 74}
Step:75	{"loss": {"learning_rate": 0.000925974851452365}, "step": 75}
Step:75	{"loss": {"pretrain_training_loss": 0.20962546702100443}, "step": 75}
Step:75	{"loss": {"pretrain_validation_loss": 1.3834121140880862}, "step": 75}
Step:76	{"loss": {"learning_rate": 0.0008898950353862994}, "step": 76}
Step:76	{"loss": {"pretrain_training_loss": 0.2104317822310118}, "step": 76}
Step:76	{"loss": {"pretrain_validation_loss": 1.295577163281648}, "step": 76}
Step:77	{"loss": {"learning_rate": 0.0008542333547875573}, "step": 77}
Step:77	{"loss": {"pretrain_training_loss": 0.20533500481604644}, "step": 77}
Step:77	{"loss": {"pretrain_validation_loss": 1.4422813852628071}, "step": 77}
Step:78	{"loss": {"learning_rate": 0.0008190142503906795}, "step": 78}
Step:78	{"loss": {"pretrain_training_loss": 0.2101443215761486}, "step": 78}
Step:78	{"loss": {"pretrain_validation_loss": 1.451144411943961}, "step": 78}
Step:79	{"loss": {"learning_rate": 0.000784261859610587}, "step": 79}
Step:79	{"loss": {"pretrain_training_loss": 0.2022813223440851}, "step": 79}
Step:79	{"loss": {"pretrain_validation_loss": 1.3601509280826733}, "step": 79}
Step:80	{"loss": {"learning_rate": 0.00075}, "step": 80}
Step:80	{"loss": {"pretrain_training_loss": 0.19982728068491784}, "step": 80}
Step:80	{"loss": {"pretrain_validation_loss": 1.3726819857307102}, "step": 80}
Step:81	{"loss": {"learning_rate": 0.0007162521529260764}, "step": 81}
Step:81	{"loss": {"pretrain_training_loss": 0.1991953797444535}, "step": 81}
Step:81	{"loss": {"pretrain_validation_loss": 1.5925966760386592}, "step": 81}
Step:82	{"loss": {"learning_rate": 0.0006830414474774595}, "step": 82}
Step:82	{"loss": {"pretrain_training_loss": 0.2030474568222092}, "step": 82}
Step:82	{"loss": {"pretrain_validation_loss": 1.4461158168488655}, "step": 82}
Step:83	{"loss": {"learning_rate": 0.00065039064461275}, "step": 83}
Step:83	{"loss": {"pretrain_training_loss": 0.19766442020697222}, "step": 83}
Step:83	{"loss": {"pretrain_validation_loss": 1.4392706432204316}, "step": 83}
Step:84	{"loss": {"learning_rate": 0.0006183221215612901}, "step": 84}
Step:84	{"loss": {"pretrain_training_loss": 0.20094406496747275}, "step": 84}
Step:84	{"loss": {"pretrain_validation_loss": 1.565054980741031}, "step": 84}
Step:85	{"loss": {"learning_rate": 0.0005868578564869187}, "step": 85}
Step:85	{"loss": {"pretrain_training_loss": 0.1998810848726659}, "step": 85}
Step:85	{"loss": {"pretrain_validation_loss": 1.447734277749407}, "step": 85}
Step:86	{"loss": {"learning_rate": 0.0005560194134252438}, "step": 86}
Step:86	{"loss": {"pretrain_training_loss": 0.1994188178649179}, "step": 86}
Step:86	{"loss": {"pretrain_validation_loss": 1.568699676921402}, "step": 86}
Step:87	{"loss": {"learning_rate": 0.0005258279275047245}, "step": 87}
Step:87	{"loss": {"pretrain_training_loss": 0.19427822782315285}, "step": 87}
Step:87	{"loss": {"pretrain_validation_loss": 1.3848007589146711}, "step": 87}
Step:88	{"loss": {"learning_rate": 0.0004963040904617129}, "step": 88}
Step:88	{"loss": {"pretrain_training_loss": 0.19853623388646705}, "step": 88}
Step:88	{"loss": {"pretrain_validation_loss": 1.3310686876808389}, "step": 88}
Step:89	{"loss": {"learning_rate": 0.00046746813645936856}, "step": 89}
Step:89	{"loss": {"pretrain_training_loss": 0.19378956196808902}, "step": 89}
Step:89	{"loss": {"pretrain_validation_loss": 1.4576727125955664}, "step": 89}
Step:90	{"loss": {"learning_rate": 0.00043933982822017855}, "step": 90}
Step:90	{"loss": {"pretrain_training_loss": 0.19392938961770012}, "step": 90}
Step:90	{"loss": {"pretrain_validation_loss": 1.4290346252745476}, "step": 90}
Step:91	{"loss": {"learning_rate": 0.0004119384434815683}, "step": 91}
Step:91	{"loss": {"pretrain_training_loss": 0.1927210277562691}, "step": 91}
Step:91	{"loss": {"pretrain_validation_loss": 1.4539812354074009}, "step": 91}
Step:92	{"loss": {"learning_rate": 0.0003852827617839086}, "step": 92}
Step:92	{"loss": {"pretrain_training_loss": 0.19409411583358913}, "step": 92}
Step:92	{"loss": {"pretrain_validation_loss": 1.4082899387331977}, "step": 92}
Step:93	{"loss": {"learning_rate": 0.00035939105159995345}, "step": 93}
Step:93	{"loss": {"pretrain_training_loss": 0.19112304094001706}, "step": 93}
Step:93	{"loss": {"pretrain_validation_loss": 1.3528676482214443}, "step": 93}
Step:94	{"loss": {"learning_rate": 0.00033428105781454354}, "step": 94}
Step:94	{"loss": {"pretrain_training_loss": 0.19539532771673343}, "step": 94}
Step:94	{"loss": {"pretrain_validation_loss": 1.5281241397926772}, "step": 94}
Step:95	{"loss": {"learning_rate": 0.000309969989563147}, "step": 95}
Step:95	{"loss": {"pretrain_training_loss": 0.1893456592657309}, "step": 95}
Step:95	{"loss": {"pretrain_validation_loss": 1.5873198569684788}, "step": 95}
Step:96	{"loss": {"learning_rate": 0.00028647450843757887}, "step": 96}
Step:96	{"loss": {"pretrain_training_loss": 0.18741287883772725}, "step": 96}
Step:96	{"loss": {"pretrain_validation_loss": 1.509744884311289}, "step": 96}
Step:97	{"loss": {"learning_rate": 0.00026381071706697666}, "step": 97}
Step:97	{"loss": {"pretrain_training_loss": 0.18897260938767607}, "step": 97}
Step:97	{"loss": {"pretrain_validation_loss": 1.4435433136380238}, "step": 97}
Step:98	{"loss": {"learning_rate": 0.00024199414808186398}, "step": 98}
Step:98	{"loss": {"pretrain_training_loss": 0.18993315264084082}, "step": 98}
Step:98	{"loss": {"pretrain_validation_loss": 1.4069184349930806}, "step": 98}
Step:99	{"loss": {"learning_rate": 0.00022103975346886163}, "step": 99}
Step:99	{"loss": {"pretrain_training_loss": 0.18766113079835048}, "step": 99}
Step:99	{"loss": {"pretrain_validation_loss": 1.45611916590428}, "step": 99}
Step:100	{"loss": {"learning_rate": 0.00020096189432334185}, "step": 100}
Step:100	{"loss": {"pretrain_training_loss": 0.1855704595774523}, "step": 100}
Step:100	{"loss": {"pretrain_validation_loss": 1.3890297067338142}, "step": 100}
Step:101	{"loss": {"learning_rate": 0.000181774331007052}, "step": 101}
Step:101	{"loss": {"pretrain_training_loss": 0.18649015121411214}, "step": 101}
Step:101	{"loss": {"pretrain_validation_loss": 1.3612573777419934}, "step": 101}
Step:102	{"loss": {"learning_rate": 0.00016349021371744825}, "step": 102}
Step:102	{"loss": {"pretrain_training_loss": 0.18619274746862044}, "step": 102}
Step:102	{"loss": {"pretrain_validation_loss": 1.3582985440026158}, "step": 102}
Step:103	{"loss": {"learning_rate": 0.00014612207347520934}, "step": 103}
Step:103	{"loss": {"pretrain_training_loss": 0.18475070735612767}, "step": 103}
Step:103	{"loss": {"pretrain_validation_loss": 1.3638624056525852}, "step": 103}
Step:104	{"loss": {"learning_rate": 0.0001296818135360985}, "step": 104}
Step:104	{"loss": {"pretrain_training_loss": 0.18493668319567427}, "step": 104}
Step:104	{"loss": {"pretrain_validation_loss": 1.4046896687452344}, "step": 104}
Step:105	{"loss": {"learning_rate": 0.00011418070123306987}, "step": 105}
Step:105	{"loss": {"pretrain_training_loss": 0.18180710152068547}, "step": 105}
Step:105	{"loss": {"pretrain_validation_loss": 1.382729703101559}, "step": 105}
Step:106	{"loss": {"learning_rate": 9.962936025419735e-05}, "step": 106}
Step:106	{"loss": {"pretrain_training_loss": 0.18479248089196515}, "step": 106}
Step:106	{"loss": {"pretrain_validation_loss": 1.4180638327978659}, "step": 106}
Step:107	{"loss": {"learning_rate": 8.60377633617325e-05}, "step": 107}
Step:107	{"loss": {"pretrain_training_loss": 0.18214701207486228}, "step": 107}
Step:107	{"loss": {"pretrain_validation_loss": 1.424181108025537}, "step": 107}
Step:108	{"loss": {"learning_rate": 7.341522555726968e-05}, "step": 108}
Step:108	{"loss": {"pretrain_training_loss": 0.18431703699233365}, "step": 108}
Step:108	{"loss": {"pretrain_validation_loss": 1.4249679282091665}, "step": 108}
Step:109	{"loss": {"learning_rate": 6.177039769771057e-05}, "step": 109}
Step:109	{"loss": {"pretrain_training_loss": 0.17804355829344806}, "step": 109}
Step:109	{"loss": {"pretrain_validation_loss": 1.3895290770392488}, "step": 109}
Step:110	{"loss": {"learning_rate": 5.111126056639751e-05}, "step": 110}
Step:110	{"loss": {"pretrain_training_loss": 0.17623149947052108}, "step": 110}
Step:110	{"loss": {"pretrain_validation_loss": 1.4262777091800303}, "step": 110}
Step:111	{"loss": {"learning_rate": 4.144511940348515e-05}, "step": 111}
Step:111	{"loss": {"pretrain_training_loss": 0.1815524759899728}, "step": 111}
Step:111	{"loss": {"pretrain_validation_loss": 1.376994656479877}, "step": 111}
Step:112	{"loss": {"learning_rate": 3.277859889929146e-05}, "step": 112}
Step:112	{"loss": {"pretrain_training_loss": 0.18013220906368418}, "step": 112}
Step:112	{"loss": {"pretrain_validation_loss": 1.4338010584098706}, "step": 112}
Step:113	{"loss": {"learning_rate": 2.5117638654068232e-05}, "step": 113}
Step:113	{"loss": {"pretrain_training_loss": 0.1815901950112506}, "step": 113}
Step:113	{"loss": {"pretrain_validation_loss": 1.420248441938041}, "step": 113}
Step:114	{"loss": {"learning_rate": 1.8467489107293506e-05}, "step": 114}
Step:114	{"loss": {"pretrain_training_loss": 0.1811985043898834}, "step": 114}
Step:114	{"loss": {"pretrain_validation_loss": 1.39280920356944}, "step": 114}
Step:115	{"loss": {"learning_rate": 1.2832707939284424e-05}, "step": 115}
Step:115	{"loss": {"pretrain_training_loss": 0.17898653647048765}, "step": 115}
Step:115	{"loss": {"pretrain_validation_loss": 1.3953822497008503}, "step": 115}
Step:116	{"loss": {"learning_rate": 8.217156947589898e-06}, "step": 116}
Step:116	{"loss": {"pretrain_training_loss": 0.18010506460436215}, "step": 116}
Step:116	{"loss": {"pretrain_validation_loss": 1.3858357667922974}, "step": 116}
Step:117	{"loss": {"learning_rate": 4.623999400308054e-06}, "step": 117}
Step:117	{"loss": {"pretrain_training_loss": 0.18192890064867012}, "step": 117}
Step:117	{"loss": {"pretrain_validation_loss": 1.3887145467426465}, "step": 117}
Step:118	{"loss": {"learning_rate": 2.05569786813925e-06}, "step": 118}
Step:118	{"loss": {"pretrain_training_loss": 0.18014488504943352}, "step": 118}
Step:118	{"loss": {"pretrain_validation_loss": 1.423368165458458}, "step": 118}
Step:119	{"loss": {"learning_rate": 5.140125366641102e-07}, "step": 119}
Step:119	{"loss": {"pretrain_training_loss": 0.1774939278410713}, "step": 119}
Step:119	{"loss": {"pretrain_validation_loss": 1.4477040275283481}, "step": 119}
